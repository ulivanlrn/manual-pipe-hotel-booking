{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8caa2a0",
   "metadata": {},
   "source": [
    "# Part 1. Studying the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "5d104310",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T16:32:44.157050Z",
     "start_time": "2025-06-25T16:32:44.155506Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import warnings"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "4d984d9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T16:32:49.907650Z",
     "start_time": "2025-06-25T16:32:49.904945Z"
    }
   },
   "source": [
    "pd.options.display.max_columns = 100\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "65ba35a7",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-06-25T16:32:58.609319Z",
     "start_time": "2025-06-25T16:32:58.478251Z"
    }
   },
   "source": "raw_data = pd.read_csv('../data/hotel_bookings.csv')",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "b9fc9de7",
   "metadata": {},
   "source": [
    "    We will consider booking without any adult irrelevant."
   ]
  },
  {
   "cell_type": "code",
   "id": "f90b083f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T16:33:02.007458Z",
     "start_time": "2025-06-25T16:33:01.993788Z"
    }
   },
   "source": [
    "data = raw_data[raw_data['adults']>0]"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "cc16985e",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "id": "663aee88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T16:33:03.872241Z",
     "start_time": "2025-06-25T16:33:03.840477Z"
    }
   },
   "source": [
    "((data.isnull().sum()\n",
    " /len(raw_data)*100)\n",
    " .pipe(lambda s: s[s>=50])\n",
    ")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "company    93.995309\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "681e3cb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T16:33:08.273574Z",
     "start_time": "2025-06-25T16:33:08.253039Z"
    }
   },
   "source": [
    "# Drop 'company' as 94% of values are missing\n",
    "\n",
    "cols_to_drop = ['company']  # we will place all unnecessary features in this list\n",
    "data = data.drop(['company'], axis=1)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "e146aee5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T16:33:09.535362Z",
     "start_time": "2025-06-25T16:33:09.532711Z"
    }
   },
   "source": [
    "# columns which we likely need to drop\n",
    "drop_candidates = []"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "3c137fc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T16:33:10.859972Z",
     "start_time": "2025-06-25T16:33:10.791497Z"
    }
   },
   "source": [
    "data.isna().sum()[data.isna().sum()>0]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "children        4\n",
       "country       478\n",
       "agent       16263\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "9efb685a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T16:33:21.930171Z",
     "start_time": "2025-06-25T16:33:21.911759Z"
    }
   },
   "source": [
    "data['reservation_status_date'] = pd.to_datetime(data['reservation_status_date'])"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "9f2c7378",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T16:33:22.983977Z",
     "start_time": "2025-06-25T16:33:22.978359Z"
    }
   },
   "source": [
    "data.dtypes.value_counts()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int64             16\n",
       "object            11\n",
       "float64            3\n",
       "datetime64[ns]     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "97228f69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T16:33:33.148792Z",
     "start_time": "2025-06-25T16:33:33.140241Z"
    }
   },
   "source": "object_cols = data.select_dtypes('object').columns",
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "4991b515",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2025-06-25T16:33:35.043397Z",
     "start_time": "2025-06-25T16:33:35.006932Z"
    }
   },
   "source": [
    "# checking correctness of values in categorical features\n",
    "\n",
    "for col in object_cols:\n",
    "    print('Feature {} contains values {}'.format(col, data[col].unique()))\n",
    "    print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature hotel contains values ['Resort Hotel' 'City Hotel']\n",
      "\n",
      "Feature arrival_date_month contains values ['July' 'August' 'September' 'October' 'November' 'December' 'January'\n",
      " 'February' 'March' 'April' 'May' 'June']\n",
      "\n",
      "Feature meal contains values ['BB' 'FB' 'HB' 'SC' 'Undefined']\n",
      "\n",
      "Feature country contains values ['PRT' 'GBR' 'USA' 'ESP' 'IRL' 'FRA' nan 'ROU' 'NOR' 'OMN' 'ARG' 'POL'\n",
      " 'DEU' 'BEL' 'CHE' 'CN' 'GRC' 'ITA' 'NLD' 'DNK' 'RUS' 'SWE' 'AUS' 'EST'\n",
      " 'CZE' 'BRA' 'FIN' 'MOZ' 'BWA' 'LUX' 'SVN' 'ALB' 'IND' 'CHN' 'MEX' 'MAR'\n",
      " 'UKR' 'SMR' 'LVA' 'PRI' 'SRB' 'CHL' 'AUT' 'BLR' 'LTU' 'TUR' 'ZAF' 'AGO'\n",
      " 'ISR' 'CYM' 'ZMB' 'CPV' 'ZWE' 'DZA' 'KOR' 'CRI' 'HUN' 'ARE' 'TUN' 'JAM'\n",
      " 'HRV' 'HKG' 'IRN' 'GEO' 'AND' 'GIB' 'URY' 'JEY' 'CAF' 'CYP' 'COL' 'GGY'\n",
      " 'KWT' 'NGA' 'MDV' 'VEN' 'SVK' 'FJI' 'KAZ' 'PAK' 'IDN' 'LBN' 'PHL' 'SEN'\n",
      " 'SYC' 'AZE' 'BHR' 'NZL' 'THA' 'DOM' 'MKD' 'MYS' 'ARM' 'JPN' 'LKA' 'CUB'\n",
      " 'CMR' 'BIH' 'MUS' 'COM' 'SUR' 'UGA' 'BGR' 'CIV' 'JOR' 'SYR' 'SGP' 'BDI'\n",
      " 'SAU' 'VNM' 'PLW' 'QAT' 'EGY' 'PER' 'MLT' 'MWI' 'ECU' 'MDG' 'ISL' 'UZB'\n",
      " 'NPL' 'BHS' 'MAC' 'TGO' 'TWN' 'DJI' 'STP' 'KNA' 'ETH' 'IRQ' 'HND' 'RWA'\n",
      " 'KHM' 'MCO' 'BGD' 'IMN' 'TJK' 'NIC' 'BEN' 'VGB' 'TZA' 'GAB' 'GHA' 'TMP'\n",
      " 'GLP' 'KEN' 'LIE' 'GNB' 'MNE' 'UMI' 'MYT' 'FRO' 'MMR' 'PAN' 'BFA' 'LBY'\n",
      " 'MLI' 'NAM' 'BOL' 'PRY' 'BRB' 'ABW' 'AIA' 'SLV' 'DMA' 'PYF' 'GUY' 'LCA'\n",
      " 'ATA' 'GTM' 'ASM' 'MRT' 'NCL' 'KIR' 'SDN' 'ATF' 'SLE' 'LAO']\n",
      "\n",
      "Feature market_segment contains values ['Direct' 'Corporate' 'Online TA' 'Offline TA/TO' 'Complementary' 'Groups'\n",
      " 'Undefined' 'Aviation']\n",
      "\n",
      "Feature distribution_channel contains values ['Direct' 'Corporate' 'TA/TO' 'Undefined' 'GDS']\n",
      "\n",
      "Feature reserved_room_type contains values ['C' 'A' 'D' 'E' 'G' 'F' 'H' 'L' 'B']\n",
      "\n",
      "Feature assigned_room_type contains values ['C' 'A' 'D' 'E' 'G' 'F' 'I' 'B' 'H' 'L' 'K']\n",
      "\n",
      "Feature deposit_type contains values ['No Deposit' 'Refundable' 'Non Refund']\n",
      "\n",
      "Feature customer_type contains values ['Transient' 'Contract' 'Transient-Party' 'Group']\n",
      "\n",
      "Feature reservation_status contains values ['Check-Out' 'Canceled' 'No-Show']\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "b102b81e",
   "metadata": {},
   "source": [
    "    Our data doesn`t contain any obvious dirtiness. We only have to deal with 3 features containing missing values. "
   ]
  },
  {
   "cell_type": "code",
   "id": "ffe1eeb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T16:33:39.085926Z",
     "start_time": "2025-06-25T16:33:39.079891Z"
    }
   },
   "source": [
    "data['children'].value_counts()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "children\n",
       "0.0     110616\n",
       "1.0       4857\n",
       "2.0       3444\n",
       "3.0         65\n",
       "10.0         1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "836d1d6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T16:33:41.664656Z",
     "start_time": "2025-06-25T16:33:41.660337Z"
    }
   },
   "source": [
    "data['children'].fillna(0, inplace=True)"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "6b6b6e32",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-06-25T16:33:45.363515Z",
     "start_time": "2025-06-25T16:33:45.352915Z"
    }
   },
   "source": [
    "data['country'].value_counts() "
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country\n",
       "PRT    48440\n",
       "GBR    12105\n",
       "FRA    10376\n",
       "ESP     8546\n",
       "DEU     7271\n",
       "       ...  \n",
       "DJI        1\n",
       "BWA        1\n",
       "HND        1\n",
       "VGB        1\n",
       "NAM        1\n",
       "Name: count, Length: 177, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "4a59a1a7",
   "metadata": {},
   "source": [
    "    Random impute - we will fill missing values with randomly sampled values of feature."
   ]
  },
  {
   "cell_type": "code",
   "id": "de84d538",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T16:34:05.399285Z",
     "start_time": "2025-06-25T16:34:05.389725Z"
    }
   },
   "source": "sample = data['country'].dropna().sample(data['country'].isna().sum())",
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "3cccdf9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T16:34:06.360113Z",
     "start_time": "2025-06-25T16:34:06.353048Z"
    }
   },
   "source": [
    "sample.index = data[data['country'].isna()].index"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "c94875f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T16:34:07.321810Z",
     "start_time": "2025-06-25T16:34:07.314684Z"
    }
   },
   "source": [
    "data.loc[data['country'].isna(), 'country'] = sample"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "132d8248",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T16:34:08.728070Z",
     "start_time": "2025-06-25T16:34:08.723879Z"
    }
   },
   "source": [
    "# automating\n",
    "def random_impute(df, feature):\n",
    "    random_sample = df[feature].dropna().sample(df[feature].isna().sum())\n",
    "    random_sample.index = df[df[feature].isna()].index\n",
    "    df.loc[df[feature].isna(), feature] = random_sample"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "4c49435b",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-06-25T16:34:10.906377Z",
     "start_time": "2025-06-25T16:34:10.901332Z"
    }
   },
   "source": [
    "data['agent'].value_counts()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "agent\n",
       "9.0      31743\n",
       "240.0    13922\n",
       "1.0       7187\n",
       "14.0      3617\n",
       "7.0       3528\n",
       "         ...  \n",
       "289.0        1\n",
       "432.0        1\n",
       "265.0        1\n",
       "93.0         1\n",
       "304.0        1\n",
       "Name: count, Length: 333, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "e24995da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T16:34:12.706254Z",
     "start_time": "2025-06-25T16:34:12.694379Z"
    }
   },
   "source": "random_impute(data, 'agent')",
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "c8e9d598",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T16:34:15.895289Z",
     "start_time": "2025-06-25T16:34:15.858798Z"
    }
   },
   "source": [
    "data.isna().sum()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hotel                             0\n",
       "is_canceled                       0\n",
       "lead_time                         0\n",
       "arrival_date_year                 0\n",
       "arrival_date_month                0\n",
       "arrival_date_week_number          0\n",
       "arrival_date_day_of_month         0\n",
       "stays_in_weekend_nights           0\n",
       "stays_in_week_nights              0\n",
       "adults                            0\n",
       "children                          0\n",
       "babies                            0\n",
       "meal                              0\n",
       "country                           0\n",
       "market_segment                    0\n",
       "distribution_channel              0\n",
       "is_repeated_guest                 0\n",
       "previous_cancellations            0\n",
       "previous_bookings_not_canceled    0\n",
       "reserved_room_type                0\n",
       "assigned_room_type                0\n",
       "booking_changes                   0\n",
       "deposit_type                      0\n",
       "agent                             0\n",
       "days_in_waiting_list              0\n",
       "customer_type                     0\n",
       "adr                               0\n",
       "required_car_parking_spaces       0\n",
       "total_of_special_requests         0\n",
       "reservation_status                0\n",
       "reservation_status_date           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "cf0062df",
   "metadata": {},
   "source": [
    "    Now we have a clean dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3dedfb",
   "metadata": {},
   "source": [
    "## Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "id": "38c994d1",
   "metadata": {},
   "source": [
    "# setting runtime configurtion\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['axes.labelsize'] = 13\n",
    "plt.rcParams['xtick.labelsize'] = 11\n",
    "plt.rcParams['ytick.labelsize'] = 11"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f85593b5",
   "metadata": {},
   "source": [
    "### Top countries where the guests come from"
   ]
  },
  {
   "cell_type": "code",
   "id": "1315c378",
   "metadata": {},
   "source": [
    "data_canceled0 = data.query(\"is_canceled==0\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "22ddc1b7",
   "metadata": {},
   "source": [
    "country_data = (data_canceled0\n",
    " .groupby(['hotel', 'country']).size()\n",
    " .reset_index()\n",
    " .rename(columns={0:'no_guests'})\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b5dc1eed",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "country_data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5c3ea99d",
   "metadata": {},
   "source": [
    "def top_n_items(sub_df, n, sort_col):\n",
    "    return sub_df.nlargest(n, sort_col)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "186e7485",
   "metadata": {},
   "source": [
    "country_data = country_data.groupby('hotel').apply(top_n_items, n=10, sort_col='no_guests')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "70b3878c",
   "metadata": {},
   "source": [
    "country_data = country_data.reset_index(level=0, drop=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0a9e79ad",
   "metadata": {},
   "source": [
    "plt.figure(figsize = (10,6))\n",
    "sns.barplot(data = country_data, x = 'country', y = 'no_guests', hue='hotel')\n",
    "plt.title('Top countries where the guests come from')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "17b41bab",
   "metadata": {},
   "source": [
    "    Portugal is the leader in the number of visitors which come from there for both hotels. In case of City hotel the second and third places belong to France and Germany respectively. For Resort hotel these are Great Britain and Spain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf09304d",
   "metadata": {},
   "source": [
    "### Do guests come with children/babies?"
   ]
  },
  {
   "cell_type": "code",
   "id": "1aea0b22",
   "metadata": {},
   "source": [
    "children = data_canceled0[['children', 'babies', 'hotel']]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1ded78c2",
   "metadata": {},
   "source": [
    "children['with_children'] = (children['children']>0)|(children['babies']>0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "63c1cff8",
   "metadata": {},
   "source": [
    "children['with_children'] = children['with_children'].map({False:'no', True:'yes'})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e23a216c",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "sns.countplot(data = children, x = 'with_children', hue='hotel')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b6a15c35",
   "metadata": {},
   "source": [
    "    Clearly, the majority of guests come to both hotels without children."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c372d92",
   "metadata": {},
   "source": [
    "### How much do guests pay for a room per night?"
   ]
  },
  {
   "cell_type": "code",
   "id": "3e9b28f8",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.boxplot(data = data_canceled0, x = 'reserved_room_type', y = 'adr', hue = 'hotel')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "084a7794",
   "metadata": {},
   "source": [
    "    The distributions of prices for different room types are very variable. Still, one can conclude that in the City hotel rooms of type G are usually more expensive compared to other types. It is clear, that there are no rooms of type H and L in the City hotel. Rooms A are usually the cheapest in the Resort hotel and are among the cheapest in the City hotel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa4b358",
   "metadata": {},
   "source": [
    "### Bookings made for weekdays weekends or both?"
   ]
  },
  {
   "cell_type": "code",
   "id": "b6bd2d8f",
   "metadata": {},
   "source": [
    "data_canceled0.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1d0fd9db",
   "metadata": {},
   "source": [
    "def stays_func(row):\n",
    "    \n",
    "    feature1 = 'stays_in_weekend_nights'\n",
    "    feature2 = 'stays_in_week_nights'\n",
    "    \n",
    "    if (row[feature1] > 0)&(row[feature2] == 0):\n",
    "        return 'just_weekend'\n",
    "    if (row[feature1] == 0)&(row[feature2] > 0):\n",
    "        return 'just_week'\n",
    "    if (row[feature1] > 0)&(row[feature2] > 0):\n",
    "        return 'both_weekend_and_week'\n",
    "    else:\n",
    "        return 'undefined'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1f9dff4c",
   "metadata": {},
   "source": [
    "data_canceled0['stays'] = data_canceled0.apply(stays_func,axis=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "68fa39a3",
   "metadata": {},
   "source": [
    "resort_data = data_canceled0.query(\"hotel=='Resort Hotel'\")\n",
    "city_data = data_canceled0.query(\"hotel=='City Hotel'\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d2b7305f",
   "metadata": {},
   "source": [
    "resort_data.groupby(['arrival_date_month', 'stays']).size().unstack()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1daac63b",
   "metadata": {},
   "source": [
    "# let us sort the obtained dataframe by month\n",
    "\n",
    "import sort_dataframeby_monthorweek as sd"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "07701fbf",
   "metadata": {},
   "source": [
    "grouped_resort = resort_data.groupby(['arrival_date_month', 'stays']).size().unstack().reset_index()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b1460dc8",
   "metadata": {},
   "source": [
    "grouped_resort = sd.Sort_Dataframeby_Month(grouped_resort,'arrival_date_month')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e3b0d5be",
   "metadata": {},
   "source": [
    "grouped_resort = grouped_resort.set_index('arrival_date_month')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fd1fe208",
   "metadata": {},
   "source": [
    "grouped_resort"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "61604dd4",
   "metadata": {},
   "source": [
    "# the same procedure for City Hotel\n",
    "grouped_city = city_data.groupby(['arrival_date_month', 'stays']).size().unstack().reset_index()\n",
    "grouped_city = sd.Sort_Dataframeby_Month(grouped_city,'arrival_date_month')\n",
    "grouped_city = grouped_city.set_index('arrival_date_month')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d907a3c6",
   "metadata": {},
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16,14), sharex = True)\n",
    "grouped_resort.plot(kind='bar', stacked=True, ax=ax1)\n",
    "grouped_city.plot(kind='bar', stacked=True, ax=ax2)\n",
    "ax1.set_title('Resort Hotel')\n",
    "ax2.set_title('City Hotel')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d9dc4380",
   "metadata": {},
   "source": [
    "    Visitors of both hotels much less likely stay only during the weekend. Comparing graphs, it seems that blue color dominates in case of the Resort hotel. Blue corresponds to the longer stays (during both week and weekend). It is likely to be due to the Resort hotel's specialization, which is hinted at by the name itself. This also justifies why July and August are clearly the busiest months for the Resort hotel. From above graphs we can also capture, that overall number of guests in the City hotel is comparably higher in every month."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921dd7bc",
   "metadata": {},
   "source": [
    "### Which are the most busy months?"
   ]
  },
  {
   "cell_type": "code",
   "id": "3534939d",
   "metadata": {},
   "source": [
    "data_canceled0.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "579fb88e",
   "metadata": {},
   "source": [
    "month_resort = resort_data['arrival_date_month'].value_counts().reset_index()\n",
    "month_resort.columns = ['month', 'no_bookings_resort']\n",
    "month_city = city_data['arrival_date_month'].value_counts().reset_index()\n",
    "month_city.columns = ['month', 'no_bookings_city']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d280a942",
   "metadata": {},
   "source": [
    "month_data = month_resort.merge(month_city, on = 'month')\n",
    "month_data = sd.Sort_Dataframeby_Month(month_data, 'month')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "66696d6d",
   "metadata": {},
   "source": [
    "month_data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3ef7f3d2",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(month_data['month'], month_data['no_bookings_resort'], label='Resort Hotel')\n",
    "plt.plot(month_data['month'], month_data['no_bookings_city'], label='City Hotel')\n",
    "plt.ylabel('number of bookings')\n",
    "plt.xticks(rotation = 45)\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "295dd373",
   "metadata": {},
   "source": [
    "    The dynamics for both hotels is simple - generally the winter period is quieter and summer time is the busiest. However, unlike City hotel, the rise and descent of activity for the Resort hotel is not so smooth with abrupt drops in June and September. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946c4fb0",
   "metadata": {},
   "source": [
    "### Which month has highest adr?"
   ]
  },
  {
   "cell_type": "code",
   "id": "edad023a",
   "metadata": {},
   "source": [
    "data_canceled0_sorted = sd.Sort_Dataframeby_Month(data_canceled0, 'arrival_date_month')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "90f211b4",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(data = data_canceled0_sorted, x = 'arrival_date_month', y = 'adr', hue = 'hotel')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "18070851",
   "metadata": {},
   "source": [
    "    We have already found out that the busiest period for both hotels is summer. Increase of demand leads to raise of the prices. It is especially noticable for the Resort hotel, for which seasonality plays crucial role due to its specialization. The prices in the City hotel change not as severely, although similar pattern is observed for it too. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b44b2c",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "id": "5e84f276",
   "metadata": {},
   "source": [
    "data.dtypes.value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b0df0549",
   "metadata": {},
   "source": [
    "data.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9b75e71f",
   "metadata": {},
   "source": [
    "# create lists for new features and for features which were used to form the new features\n",
    "added_cols = []\n",
    "forming_cols = []"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "11b1b568",
   "metadata": {},
   "source": [
    "data['total_nights'] = data['stays_in_weekend_nights'] + data['stays_in_week_nights']\n",
    "\n",
    "added_cols.append('total_nights')\n",
    "forming_cols.append('stays_in_weekend_nights')\n",
    "drop_candidates.append('stays_in_weekend_nights')\n",
    "forming_cols.append('stays_in_week_nights')\n",
    "drop_candidates.append('stays_in_week_nights')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c17d1f34",
   "metadata": {},
   "source": [
    "def stays_func(row):\n",
    "    \n",
    "    feature1 = 'stays_in_weekend_nights'\n",
    "    feature2 = 'stays_in_week_nights'\n",
    "    \n",
    "    if (row[feature1] > 0)&(row[feature2] == 0):\n",
    "        return 'just_weekend'\n",
    "    if (row[feature1] == 0)&(row[feature2] > 0):\n",
    "        return 'just_week'\n",
    "    if (row[feature1] > 0)&(row[feature2] > 0):\n",
    "        return 'both_weekend_and_week'\n",
    "    else:\n",
    "        return 'undefined'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "79eb1dfe",
   "metadata": {},
   "source": [
    "data['stays_format'] = data.apply(stays_func, axis=1)\n",
    "\n",
    "added_cols.append('stays_format')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9cce1ec5",
   "metadata": {},
   "source": [
    "data['total_guests'] = data['adults']+data['children']+data['babies']\n",
    "\n",
    "added_cols.append('total_guests')\n",
    "forming_cols.append('adults')\n",
    "forming_cols.append('children')\n",
    "forming_cols.append('babies')\n",
    "drop_candidates.append('adults')\n",
    "drop_candidates.append('children')\n",
    "drop_candidates.append('babies')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c738f4d5",
   "metadata": {},
   "source": [
    "def children_func(row):\n",
    "    if (row['children'] == 0)&(row['babies'] == 0):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f8ee341c",
   "metadata": {},
   "source": [
    "data['children_binary'] = data.apply(children_func, axis = 1)\n",
    "\n",
    "added_cols.append('children_binary')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9e2752a7",
   "metadata": {},
   "source": [
    "def room_type(row):\n",
    "    if row['assigned_room_type'] == row['reserved_room_type']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7d48b851",
   "metadata": {},
   "source": [
    "data['room_assigned_equal_reserved'] = data.apply(room_type, axis = 1)\n",
    "\n",
    "added_cols.append('room_assigned_equal_reserved')\n",
    "forming_cols.append('assigned_room_type')\n",
    "forming_cols.append('reserved_room_type')\n",
    "drop_candidates.append('assigned_room_type')\n",
    "drop_candidates.append('reserved_room_type')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3981dc71",
   "metadata": {},
   "source": [
    "def deposit_type(row):\n",
    "    if row['deposit_type'] == 'Non Refund':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "22e350f2",
   "metadata": {},
   "source": [
    "data['deposit_type'] = data.apply(deposit_type, axis=1)\n",
    "\n",
    "added_cols.append('deposit_type')\n",
    "forming_cols.append('deposit_type')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "701d201f",
   "metadata": {},
   "source": [
    "# represents the same info as feature is_canceled\n",
    "\n",
    "data.drop(['reservation_status'], axis=1, inplace=True)\n",
    "cols_to_drop.append('reservation_status')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ceee2880",
   "metadata": {},
   "source": [
    "data.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a29e990b",
   "metadata": {},
   "source": [
    "### Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "id": "cce47e83",
   "metadata": {},
   "source": [
    "cat_cols = data.select_dtypes('object').columns\n",
    "num_cols = data.select_dtypes(exclude=['object', 'datetime64[ns]']).columns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a1a3014e",
   "metadata": {},
   "source": [
    "data[cat_cols].nunique()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "913794ba",
   "metadata": {},
   "source": [
    "high_cardinality_feats = data[cat_cols].nunique().pipe(lambda s: s[s>=5]).index\n",
    "low_cardinality_feats = cat_cols.difference(high_cardinality_feats)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "08923ee1",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "from sklearn import set_config"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "047c1a70",
   "metadata": {},
   "source": [
    "set_config(transform_output='pandas')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "98a728f0",
   "metadata": {},
   "source": [
    "l_enc = LabelEncoder()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e229a6b6",
   "metadata": {},
   "source": [
    "for col in low_cardinality_feats:\n",
    "    data[col] = l_enc.fit_transform(data[col])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "10724c23",
   "metadata": {},
   "source": [
    "t_enc = TargetEncoder(target_type='binary', random_state=123)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a691f15f",
   "metadata": {},
   "source": [
    "data[high_cardinality_feats] = t_enc.fit_transform(data[high_cardinality_feats], data['is_canceled'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "29b9c6a3",
   "metadata": {},
   "source": [
    "data[cat_cols].head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "176cb96c",
   "metadata": {},
   "source": [
    "## Handling outliers"
   ]
  },
  {
   "cell_type": "code",
   "id": "39c2802d",
   "metadata": {},
   "source": [
    "len(num_cols)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d52416c2",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(10,3, figsize = (16,26))\n",
    "\n",
    "for i, col in enumerate(num_cols):\n",
    "    sns.histplot(data = data, x = col, ax = ax[i//3, i%3], stat='density', kde=True)\n",
    "\n",
    "fig.tight_layout()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ac053722",
   "metadata": {},
   "source": [
    "drop_candidates.extend(['arrival_date_year', 'arrival_date_month', 'arrival_date_week_number', \n",
    "                        'arrival_date_day_of_month', 'reservation_status_date'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5458f368",
   "metadata": {},
   "source": [
    "#### Continious features"
   ]
  },
  {
   "cell_type": "code",
   "id": "9c91a9c6",
   "metadata": {},
   "source": [
    "def hist_and_box(col):\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize = (8,6))\n",
    "    sns.boxplot(data[col], ax = ax1, orient = 'h')\n",
    "    sns.histplot(data[col], stat='density', kde = True, ax = ax2)\n",
    "    ax1.set_xticks([])\n",
    "    ax1.set_yticks([])\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1e8a5d75",
   "metadata": {},
   "source": [
    "hist_and_box('lead_time')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "19c4facb",
   "metadata": {},
   "source": [
    "def log_transform(col):\n",
    "    data[col] = np.log1p(data[col])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b2abb94f",
   "metadata": {},
   "source": [
    "log_transform('lead_time')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dc4302dd",
   "metadata": {},
   "source": [
    "hist_and_box('lead_time')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "69ee124a",
   "metadata": {},
   "source": [
    "#### adr"
   ]
  },
  {
   "cell_type": "code",
   "id": "b8369b6e",
   "metadata": {},
   "source": [
    "hist_and_box('adr')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "08767f7a",
   "metadata": {},
   "source": [
    "(data['adr'] < 0).sum()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1bca4612",
   "metadata": {},
   "source": [
    "    One of instances has negative adr."
   ]
  },
  {
   "cell_type": "code",
   "id": "ee73a01c",
   "metadata": {},
   "source": [
    "log_transform('adr')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4e276a63",
   "metadata": {},
   "source": [
    "data.adr.isna().sum()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e8d3df4b",
   "metadata": {},
   "source": [
    "data['adr'] = data['adr'].fillna(0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "49f7eb27",
   "metadata": {},
   "source": [
    "hist_and_box('adr')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7491e67a",
   "metadata": {},
   "source": [
    "#### Discrete features"
   ]
  },
  {
   "cell_type": "code",
   "id": "aa0e4146",
   "metadata": {},
   "source": [
    "drop_candidates"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "96af9486",
   "metadata": {},
   "source": [
    "data['total_guests'].value_counts().sort_index()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a55c0837",
   "metadata": {},
   "source": [
    "data['total_guests'] = np.where(data['total_guests']>=4, 4, data['total_guests'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "14f8f5cd",
   "metadata": {},
   "source": [
    "hist_and_box('total_guests')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b22b7fab",
   "metadata": {},
   "source": [
    "    Now value 4 in the column total_guests represents all instances for which number of total guest is equal to 4 or bigger than 4. We will apply the same procedure to other discrete features."
   ]
  },
  {
   "cell_type": "code",
   "id": "f3a2c4f5",
   "metadata": {},
   "source": [
    "def outlier_func(col, val):\n",
    "    data[col] = np.where(data[col]>=val, val, data[col])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4aba6792",
   "metadata": {},
   "source": [
    "outlier_func('previous_cancellations', 1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b67c5cb0",
   "metadata": {},
   "source": [
    "outlier_func('previous_bookings_not_canceled', 1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d1aa09e8",
   "metadata": {},
   "source": [
    "outlier_func('booking_changes', 1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "14f7e936",
   "metadata": {},
   "source": [
    "outlier_func('days_in_waiting_list', 1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "10d95efe",
   "metadata": {},
   "source": [
    "outlier_func('required_car_parking_spaces', 1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f67513f8",
   "metadata": {},
   "source": [
    "outlier_func('total_of_special_requests', 2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "322e41d7",
   "metadata": {},
   "source": [
    "outlier_func('total_nights', 8)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7d2ab2dd",
   "metadata": {},
   "source": [
    "## Filtering Features"
   ]
  },
  {
   "cell_type": "code",
   "id": "26dc9b3d",
   "metadata": {},
   "source": [
    "final_data = data.drop(drop_candidates, axis=1)\n",
    "len(final_data.columns)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8ec95778",
   "metadata": {},
   "source": [
    "#### In which columns does a single value predominate?"
   ]
  },
  {
   "cell_type": "code",
   "id": "ebe62013",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "value_list = []\n",
    "perc_list = []\n",
    "for col in final_data.columns:\n",
    "    counts = final_data[col].value_counts()/len(final_data)*100\n",
    "    value_list.append(counts.index[0])\n",
    "    perc_list.append(counts.max())\n",
    "\n",
    "share_df = (pd.DataFrame({'value': value_list, 'percentage': perc_list}, index=final_data.columns)\n",
    "           .sort_values(by='percentage', ascending=False))\n",
    "\n",
    "cols_with_single = share_df[share_df['percentage']>=90].index"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "84492d8b",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "share_df.loc[cols_with_single,:]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a45be0ea",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(figsize=(16,12))\n",
    "for i, col in enumerate(cols_with_single):\n",
    "    plt.subplot(3,2,i+1)\n",
    "    sns.kdeplot(data = final_data, x = col, hue = 'is_canceled', fill = True)\n",
    "fig.tight_layout()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "13ec5a4d",
   "metadata": {},
   "source": [
    "    Let us have a look at kdeplot for previous_bookings_not_canceled column above. It takes only two values, 0 and 1, what is true for all depicted columns. The bell above the value 1 is almost totally blue with a little share of orange color near the bottom. It means that, if the feature takes value 1 for some instance, this instance most likely takes value 0 in the target feature is_canceled. Thus, thanks to the feature previous_bookings_not_canceled, we are able to roughly separate certain amount of instances with is_canceled==0 by one criterion. If we consider the feature children_binary, it is clear there is no way to separate any group of instances with its' help. is_canceled takes 0 with approximately same probability independent from value which children_binary takes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f5c972",
   "metadata": {},
   "source": [
    "    Based on such reasoning we add 3 columns to the drop_candidates list."
   ]
  },
  {
   "cell_type": "code",
   "id": "37825eca",
   "metadata": {},
   "source": [
    "drop_candidates.extend(['children_binary', 'is_repeated_guest', 'days_in_waiting_list'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d96d03fd",
   "metadata": {},
   "source": [
    "final_data = data.drop(drop_candidates, axis=1)\n",
    "len(final_data.columns)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "88973b5f",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# required_car_parking_spaces\n",
    "final_data[final_data['required_car_parking_spaces'] == 1]['is_canceled'].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3f2d47a5",
   "metadata": {},
   "source": [
    "#### Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "id": "11616394",
   "metadata": {},
   "source": [
    "corr_m = np.absolute(final_data.corr())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "297376da",
   "metadata": {},
   "source": [
    "upper_triangle = corr_m.where(np.triu(np.ones(corr_m.shape)).astype(bool))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "67071d77",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "plt.figure(figsize = (13,10))\n",
    "sns.heatmap(upper_triangle, cbar=False, cmap = 'RdPu', annot=True, fmt='.2f')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "608288c3",
   "metadata": {},
   "source": [
    "    No highly correlated features detected."
   ]
  },
  {
   "cell_type": "code",
   "id": "642427b0",
   "metadata": {},
   "source": [
    "# preparing dropping lists\n",
    "cols_to_drop_later = []\n",
    "for col in drop_candidates:\n",
    "    if col in added_cols and col not in forming_cols:\n",
    "        added_cols.remove(col)\n",
    "        continue\n",
    "    if col not in forming_cols:\n",
    "        cols_to_drop.append(col)\n",
    "    else:\n",
    "        cols_to_drop_later.append(col)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8591e821",
   "metadata": {},
   "source": [
    "print(added_cols)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a0fccf6c",
   "metadata": {},
   "source": [
    "print(cols_to_drop)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ba0cd5fa",
   "metadata": {},
   "source": [
    "print(cols_to_drop_later)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "93f92489",
   "metadata": {},
   "source": [
    "## Performing ETL-Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "id": "e47782ac",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = 100"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2f6ce88d",
   "metadata": {},
   "source": "added_cols = ['total_nights', 'stays_format', 'total_guests', 'room_assigned_equal_reserved', 'deposit_type']",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "90b81106",
   "metadata": {},
   "source": [
    "cols_to_drop = ['company', 'reservation_status', 'arrival_date_week_number', \n",
    "                'arrival_date_day_of_month', 'reservation_status_date', 'is_repeated_guest', \n",
    "                'days_in_waiting_list', 'is_canceled']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cd841d12",
   "metadata": {},
   "source": [
    "cols_to_drop_later = ['stays_in_weekend_nights', 'stays_in_week_nights', 'adults', 'children', 'babies', \n",
    "                      'assigned_room_type', 'reserved_room_type', 'arrival_date_year', 'arrival_date_month']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "raw_data = pd.read_csv('../data/hotel_bookings.csv').query(\"adults>0\")",
   "id": "46619525f9407137",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f33ef10f",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn import set_config"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a01cc618",
   "metadata": {},
   "source": [
    "set_config(transform_output='pandas')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5ff74794",
   "metadata": {},
   "source": [
    "X = raw_data.drop(cols_to_drop, axis = 1)\n",
    "y = raw_data['is_canceled']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "eebbf9c0",
   "metadata": {},
   "source": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123, stratify=y)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e11318eb",
   "metadata": {},
   "source": [
    "len(X_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "36b6291f",
   "metadata": {},
   "source": [
    "len(X_train.columns)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d3775b74",
   "metadata": {},
   "source": [
    "### Transforming data"
   ]
  },
  {
   "cell_type": "code",
   "id": "0415317b",
   "metadata": {},
   "source": [
    "children_imputer = SimpleImputer(strategy='constant', fill_value=0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6f333a29",
   "metadata": {},
   "source": [
    "def random_impute(df):\n",
    "    for feature in df:\n",
    "        random_sample = df[feature].dropna().sample(df[feature].isna().sum())\n",
    "        random_sample.index = df[df[feature].isna()].index\n",
    "        df.loc[df[feature].isna(), feature] = random_sample\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7121bd15",
   "metadata": {},
   "source": [
    "imputer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('children_imputer', children_imputer, ['children']),\n",
    "        ('rand_imputer', FunctionTransformer(random_impute), ['country', 'agent'])\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    verbose_feature_names_out=False\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a5fb3ace",
   "metadata": {},
   "source": [
    "pipe = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', imputer)\n",
    "    ])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7f2e22d4",
   "metadata": {},
   "source": [
    "tmp_X = pipe.fit_transform(X_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "98b5d877",
   "metadata": {},
   "source": [
    "tmp_X.dtypes.value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8c01c3cf",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "tmp_X.isna().sum().sum()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "256e74ba",
   "metadata": {},
   "source": [
    "    So far we realized cleaning of dataset through our pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14ffab6",
   "metadata": {},
   "source": [
    "#### Adding features"
   ]
  },
  {
   "cell_type": "code",
   "id": "f336f196",
   "metadata": {},
   "source": [
    "added_cols"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1364fb32",
   "metadata": {},
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "# inheritance from these two classes (common convention)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0b751e11",
   "metadata": {},
   "source": [
    "class CreateStaysFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.feature1 = 'stays_in_weekend_nights'\n",
    "        self.feature2 = 'stays_in_week_nights'\n",
    "        \n",
    "    def stays_func(self, row):\n",
    "        if (row[self.feature1] > 0)&(row[self.feature2] == 0):\n",
    "            return 'just_weekend'\n",
    "        if (row[self.feature1] == 0)&(row[self.feature2] > 0):\n",
    "            return 'just_week'\n",
    "        if (row[self.feature1] > 0)&(row[self.feature2] > 0):\n",
    "            return 'both_weekend_and_week'\n",
    "        else:\n",
    "            return 'undefined'\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X['total_nights'] = X[self.feature1] + X[self.feature2]\n",
    "        X['stays_format'] = X.apply(self.stays_func, axis=1)\n",
    "        return X"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "20cb2d2f",
   "metadata": {},
   "source": [
    "def create_guests_feat(df):\n",
    "    df['total_guests'] = df['adults'] + df['children'] + df['babies']\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b057ca7a",
   "metadata": {},
   "source": [
    "class CreateRoomFeature(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.feature1 = 'assigned_room_type'\n",
    "        self.feature2 = 'reserved_room_type'\n",
    "        \n",
    "    def room_type(self, row):\n",
    "        if row[self.feature1] == row[self.feature2]:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X['room_assigned_equal_reserved'] = X.apply(self.room_type, axis = 1)\n",
    "        return X"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "69673794",
   "metadata": {},
   "source": [
    "class TransformDepositFeature(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.feat = 'deposit_type'\n",
    "        \n",
    "    def change_type(self, row):\n",
    "        if row[self.feat] == 'Non Refund':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X[self.feat] = X.apply(self.change_type, axis=1)\n",
    "        return X"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a225dbae",
   "metadata": {},
   "source": [
    "def drop_last_cols(df):\n",
    "    cols_to_drop_later = ['stays_in_weekend_nights', 'stays_in_week_nights', 'adults', 'children', 'babies', \n",
    "                      'assigned_room_type', 'reserved_room_type', 'arrival_date_year', 'arrival_date_month']\n",
    "    return df.drop(cols_to_drop_later, axis=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9fba63e3",
   "metadata": {},
   "source": [
    "feat_generator = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('stays_feats', CreateStaysFeatures(), ['stays_in_weekend_nights', 'stays_in_week_nights']),\n",
    "        ('guests_feat', FunctionTransformer(create_guests_feat), ['adults', 'children', 'babies']),\n",
    "        ('room_feat', CreateRoomFeature(), ['assigned_room_type', 'reserved_room_type']),\n",
    "        ('deposit_feat', TransformDepositFeature(), ['deposit_type'])\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    verbose_feature_names_out=False\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6fa64b09",
   "metadata": {},
   "source": [
    "Transform_Pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', imputer),\n",
    "        ('feat_generator', feat_generator),\n",
    "        ('drop_feats', FunctionTransformer(drop_last_cols))\n",
    "    ])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5cf6c404",
   "metadata": {},
   "source": [
    "    In spite of the model type we are going to implement further we will be loading data which is obtained as a result of above tranformations:"
   ]
  },
  {
   "cell_type": "code",
   "id": "83416d8b",
   "metadata": {},
   "source": [
    "Transform_Pipeline.fit_transform(X_train, y_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "aa1d46f9",
   "metadata": {},
   "source": [
    "#### Saving transform pipeline"
   ]
  },
  {
   "cell_type": "code",
   "id": "c1429a9c",
   "metadata": {},
   "source": [
    "import dill as pickle\n",
    "with open('Transform_pipe.pkl', 'wb') as file:\n",
    "    pickle.dump(Transform_Pipeline, file)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "47008c76",
   "metadata": {},
   "source": [
    "# Part 2. Model Building"
   ]
  },
  {
   "cell_type": "code",
   "id": "d0462efa",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import set_config\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = 100\n",
    "set_config(transform_output='pandas')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "76db008e",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, TargetEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.linear_model import LogisticRegression"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f6fe4315",
   "metadata": {},
   "source": [
    "# setting runtime configurtion\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['axes.labelsize'] = 13\n",
    "plt.rcParams['xtick.labelsize'] = 11\n",
    "plt.rcParams['ytick.labelsize'] = 11"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "07d6b21e",
   "metadata": {},
   "source": "raw_data = pd.read_csv('../data/hotel_bookings.csv').query(\"adults>0\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9c0e05eb",
   "metadata": {},
   "source": [
    "cols_to_drop = ['company', 'reservation_status', 'arrival_date_week_number', \n",
    "                'arrival_date_day_of_month', 'reservation_status_date', 'is_repeated_guest', \n",
    "                'days_in_waiting_list', 'is_canceled']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "18af1773",
   "metadata": {},
   "source": [
    "X = raw_data.drop(cols_to_drop, axis = 1)\n",
    "y = raw_data['is_canceled']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "403f77e3",
   "metadata": {},
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123, stratify=y)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "74af5378",
   "metadata": {},
   "source": [
    "import dill as pickle\n",
    "with open('Transform_pipe.pkl', 'rb') as file:\n",
    "    Transform_Pipe = pickle.load(file)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "72a318ac",
   "metadata": {},
   "source": [
    "def debug_transformer(X, name):\n",
    "    globals()[name] = X\n",
    "    return X"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8b044650",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c781e18",
   "metadata": {},
   "source": [
    "    A linear model known for its' interpretability and computational effiiciency. A very good choice if a dataset has features which are somewhat linearly separable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eaecfe",
   "metadata": {},
   "source": [
    "#### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "id": "b127cb3d",
   "metadata": {},
   "source": [
    "X_train_transformed = Transform_Pipe.fit_transform(X_train, y_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a684fa77",
   "metadata": {},
   "source": [
    "X_train_transformed.dtypes.value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f19aca14",
   "metadata": {},
   "source": [
    "cat_feats_df = X_train_transformed.select_dtypes('object')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "51db5559",
   "metadata": {},
   "source": [
    "cat_feats_df.nunique()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "130e21d6",
   "metadata": {},
   "source": [
    "high_cardinality_feats = ['country']\n",
    "low_cardinality_feats = cat_feats_df.columns.difference(high_cardinality_feats)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5451ed27",
   "metadata": {},
   "source": [
    "X_train_transformed[low_cardinality_feats].head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "30b06161",
   "metadata": {},
   "source": [
    "    As we observe low cardinality features are nominal with no sign of ordinality in them. We will apply one-hot encoding to exclude any dependency among categories in such features. We will apply target encoding to the rest to avoid curse of dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "id": "99ecf198",
   "metadata": {},
   "source": [
    "one_hot_encoder = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "target_encoder = TargetEncoder(target_type='binary', random_state=142)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2442ef9a",
   "metadata": {},
   "source": [
    "preprocessor1 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('one_hot_encoder', one_hot_encoder, low_cardinality_feats),\n",
    "        ('target_encoder', target_encoder, high_cardinality_feats)\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    verbose_feature_names_out=False\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ac93a918",
   "metadata": {},
   "source": [
    "log_reg1 = Pipeline(\n",
    "    steps=[\n",
    "        ('Transform_pipeline', Transform_Pipe),\n",
    "        ('preprocessor', preprocessor1),\n",
    "        # in this step we can obtain the result of transform by ETL_pipeline and preprocessor \n",
    "        ('debug', FunctionTransformer(debug_transformer, kw_args={'name': 'tmp_X'})),\n",
    "        ('model', LogisticRegression())\n",
    "    ])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2f0338e4",
   "metadata": {},
   "source": [
    "log_reg1.fit(X_train, y_train)\n",
    "print('Fitting done')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "21d3a358",
   "metadata": {},
   "source": [
    "tmp_X"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c27c0573",
   "metadata": {},
   "source": [
    "test_score = log_reg1.score(X_test, y_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aeeb33d3",
   "metadata": {},
   "source": [
    "print(f\"We created our first model with the accuracy score {test_score:.4f} on the test set\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ca649900",
   "metadata": {},
   "source": [
    "y_train.value_counts()/len(y_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ad5211f9",
   "metadata": {},
   "source": [
    "    As our classes are not ideally balanced, such metrics as precision, recall and balanced_accuracy would be more insightful to use."
   ]
  },
  {
   "cell_type": "code",
   "id": "ebf70d4b",
   "metadata": {},
   "source": "from sklearn.metrics import balanced_accuracy_score",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4965fd30",
   "metadata": {},
   "source": [
    "    In order for LogisticRegression to perform well, its' assumptions should be taken into consideration. More specifically, we have to deal with outliers in data, as they may affect our model's perfomance."
   ]
  },
  {
   "cell_type": "code",
   "id": "4db0d66e",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "num_cols = X_train_transformed.select_dtypes(exclude='object').columns\n",
    "len(num_cols)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "274e0678",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(5,3, figsize = (16,16))\n",
    "\n",
    "for i, col in enumerate(num_cols):\n",
    "    sns.histplot(data = X_train_transformed, x = col, ax = ax[i//3, i%3], stat='density', kde=True)\n",
    "\n",
    "fig.tight_layout()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f1356b52",
   "metadata": {},
   "source": [
    "    We will approach this problem as before, but encapsulating everything into functions."
   ]
  },
  {
   "cell_type": "code",
   "id": "919526eb",
   "metadata": {},
   "source": [
    "outlier_columns = ['total_guests', 'previous_cancellations', 'previous_bookings_not_canceled', 'booking_changes',\n",
    "                  'required_car_parking_spaces', 'total_of_special_requests', 'total_nights']\n",
    "values = [4, 1, 1, 1, 1, 2, 8]\n",
    "\n",
    "d = {}\n",
    "\n",
    "for obj in zip(outlier_columns, values):\n",
    "    d[obj[0]] = obj[1]\n",
    "\n",
    "print(d)    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "93081215",
   "metadata": {},
   "source": [
    "def outlier_func(df):\n",
    "    dictionary = {'total_guests': 4, 'previous_cancellations': 1, 'previous_bookings_not_canceled': 1, \n",
    "                  'booking_changes': 1, 'required_car_parking_spaces': 1, 'total_of_special_requests': 2, \n",
    "                  'total_nights': 8}\n",
    "    for col in df:\n",
    "        val = dictionary[col]\n",
    "        df[col] = np.where(df[col]>=val, val, df[col])\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c46ba0f0",
   "metadata": {},
   "source": [
    "def log_transform(df):\n",
    "    for col in df:\n",
    "        df[col] = np.log1p(df[col])\n",
    "    return df.fillna(0) "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c8b70a71",
   "metadata": {},
   "source": [
    "# binning feature agent\n",
    "binner = KBinsDiscretizer(n_bins=4, encode='ordinal', strategy='quantile', random_state=123)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e92b7487",
   "metadata": {},
   "source": [
    "preprocessor2 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('one_hot_encoder', one_hot_encoder, low_cardinality_feats),\n",
    "        ('target_encoder', target_encoder, high_cardinality_feats),\n",
    "        ('outliers_replace', FunctionTransformer(outlier_func), outlier_columns),\n",
    "        ('log_transform', FunctionTransformer(log_transform), ['lead_time', 'adr']),\n",
    "        ('binning', binner, ['agent'])\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    verbose_feature_names_out=False\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "32cd944e",
   "metadata": {},
   "source": [
    "X_train_preprocessed = preprocessor2.fit_transform(X_train_transformed, y_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5ff13638",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "fig, ax = plt.subplots(5,3, figsize = (16,16))\n",
    "\n",
    "for i, col in enumerate(num_cols):\n",
    "    sns.histplot(data = X_train_preprocessed, x = col, ax = ax[i//3, i%3], stat='density', kde=True)\n",
    "\n",
    "fig.tight_layout()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c7c777f0",
   "metadata": {},
   "source": [
    "    As LogisticRegression uses gradient descent to optimize feature weights, bringing the data to the similar scale would be neccessary."
   ]
  },
  {
   "cell_type": "code",
   "id": "bfbdd101",
   "metadata": {},
   "source": [
    "std_scaler = StandardScaler()\n",
    "min_max_scaler = MinMaxScaler()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6bc442c0",
   "metadata": {},
   "source": [
    "scaler2 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('std_scaler', std_scaler, ['lead_time', 'adr']),\n",
    "        ('min_max_scaler', min_max_scaler, ['total_nights', 'total_guests', \n",
    "                                            'agent', 'total_of_special_requests'])\n",
    "    ],\n",
    "    remainder = 'passthrough'\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7842469f",
   "metadata": {},
   "source": [
    "log_reg2 = Pipeline(\n",
    "    steps=[\n",
    "        ('Transform_pipeline', Transform_Pipe),\n",
    "        ('preprocessor', preprocessor2),\n",
    "        ('scaler', scaler2),\n",
    "        ('debug', FunctionTransformer(debug_transformer, kw_args={'name': 'tmp_X'})),\n",
    "        ('model', LogisticRegression(max_iter=200))\n",
    "        # we set max_iter higher than default to prevent ConvergenceWarning showing up\n",
    "    ])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b890276f",
   "metadata": {},
   "source": [
    "    So, we created a specific model building procedure which consists of different steps (transforming, preprocessing, scaling...). It is time to estimate its' generalization perfomance and for that purpose we will resort to NestedCrossValidation. This is a computationally expensive technique, however our dataset is not too big so we'll go for it. The key advantage of Nested CV is that it prevents information leakage during the training procedure leading to not overly optimistic results on the validation data."
   ]
  },
  {
   "cell_type": "code",
   "id": "b091f6b6",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3ba9e75f",
   "metadata": {},
   "source": [
    "# filtering warnings which show up because of the processes in GridSearchCV running in parallel\n",
    "import os\n",
    "\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore:DataFrameGroupBy.apply operated on the grouping columns:DeprecationWarning,\\\n",
    "                                ignore:Bins whose width are too small:UserWarning,\\\n",
    "                                ignore:invalid value encountered in log1p:RuntimeWarning,\\\n",
    "                                ignore:Found unknown categories in columns:UserWarning'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "902a34fb",
   "metadata": {},
   "source": [
    "outer_nest_cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=123)\n",
    "inner_nest_cv = StratifiedKFold(n_splits=3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8e470246",
   "metadata": {},
   "source": [
    "l1 = np.array([0.1, 0.3, 0.5, 0.7, 1])\n",
    "l2 = np.arange(5, 31, 5)\n",
    "l3 = np.arange(40, 151, 10)\n",
    "C_list = np.concatenate([l1,l2,l3])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "033c8316",
   "metadata": {},
   "source": [
    "param_grid = {'model__C': C_list}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "faf1f363",
   "metadata": {},
   "source": [
    "gscv = GridSearchCV(log_reg2, param_grid=param_grid, cv = inner_nest_cv, n_jobs=-1, scoring='neg_log_loss')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "43cdd25b",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "cv_scores = cross_validate(gscv, X, y, cv = outer_nest_cv, n_jobs=-1,\n",
    "                           scoring=['balanced_accuracy', 'precision', 'recall', 'f1']\n",
    "                          )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "06804e7f",
   "metadata": {},
   "source": [
    "log_reg2_nestedCV = pd.DataFrame(cv_scores).drop(['fit_time', 'score_time'], axis=1).agg(['mean', 'std']).T\n",
    "log_reg2_nestedCV"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "38c9206c",
   "metadata": {},
   "source": "    What this table tells us is, that by implementing such model we could expect precision and recall to be approximately 0.82 and 0.695, respectively, on the unseen data, regardless of a model generating method (by this I imply the way we split our data on train and test sets). Is it the best model building procedure? - this question still remains open."
  },
  {
   "cell_type": "code",
   "id": "dfbed817",
   "metadata": {},
   "source": [
    "# Creating NestedCV function\n",
    "def nested_cv(model):\n",
    "    outer_nest_cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=123)\n",
    "    inner_nest_cv = StratifiedKFold(n_splits=3)\n",
    "    param_grid = {'model__C': C_list}\n",
    "    \n",
    "    gscv = GridSearchCV(model, param_grid=param_grid, cv = inner_nest_cv, n_jobs=-1, scoring='neg_log_loss')\n",
    "    cv_scores = cross_validate(gscv, X, y, cv = outer_nest_cv, n_jobs=-1,\n",
    "                               scoring=['balanced_accuracy', 'precision', 'recall', 'f1']\n",
    "                              )\n",
    "    return(pd.DataFrame(cv_scores).drop(['fit_time', 'score_time'], axis=1).agg(['mean', 'std']).T)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c71a1b8c",
   "metadata": {},
   "source": [
    "#### Personal remark concearning the significance of precision and recall for this particular classification problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f762b472",
   "metadata": {},
   "source": [
    "The main aim is to understand in advance whether a booking will be canceled (1) or not (0). I would assume that enhancing precision metric should be more prioritized over finding the best trade-off between precision and recall. When precision increases it implies, that we get less FalsePositive predictions. In other words, we are more likely to predict 0 when 1 is a true value rather than 1 instead 0, which I find good for the hotel bussiness. The reasoning behind: \n",
    "- It is a good idea to take measures towards those guests who are in question. These could be keeping them interested to come (offering special deals, discounts e.t.c) or maybe being prepared to offer the booked room to other people (so that it remains occupied on these dates).\n",
    "- However, in my opinion, these measures are applicable only with strong certainty of cancellation, otherwise these actions could induce inconviniences or income loss.\n",
    "- With that said, it is not a huge problem to miss some cancellations (have lower recall), but it is more profitable to be very certain about cancellation (less false positives --> higher precision).\n",
    "\n",
    "In terms of our model, predicting 0 correctly is more important than predicting 1 correctly. Thus, weight of class 0 should be greater in magnitude. The following two cells demonstrate the impact of such class weights tweaking:"
   ]
  },
  {
   "cell_type": "code",
   "id": "24a28245",
   "metadata": {},
   "source": [
    "log_reg2_weights = Pipeline(\n",
    "    steps=[\n",
    "        ('Transform_pipeline', Transform_Pipe),\n",
    "        ('preprocessor', preprocessor2),\n",
    "        ('scaler', scaler2),\n",
    "        ('debug', FunctionTransformer(debug_transformer, kw_args={'name': 'tmp_X'})),\n",
    "        ('model', LogisticRegression(max_iter=200, class_weight={0: 1, 1: 0.5}))\n",
    "    ])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "39ea13fa",
   "metadata": {},
   "source": [
    "log_reg2_weights_nestedCV = nested_cv(log_reg2_weights)\n",
    "log_reg2_weights_nestedCV"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "663d06b1",
   "metadata": {},
   "source": [
    "As expected, we get essential decreasement of recall and, as a consequence, f1-score metrics. However, for this price we obtain better precision score, exactly what is desired.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10ed02e",
   "metadata": {},
   "source": [
    "    Another useful step would be plotting Learning curves for our model. This is a good method to check sanity of the model and get ideas what could potentially improve its' perfomance."
   ]
  },
  {
   "cell_type": "code",
   "id": "5fad6167",
   "metadata": {},
   "source": [
    "cv = StratifiedKFold()\n",
    "gscv = GridSearchCV(log_reg2, param_grid=param_grid, cv = cv, n_jobs=-1, scoring='neg_log_loss')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bd167a61",
   "metadata": {},
   "source": [
    "gscv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Done\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "29124a65",
   "metadata": {},
   "source": [
    "model = gscv.best_estimator_"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f21c2fcc",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import LearningCurveDisplay"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "50531bb7",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(14,6))\n",
    "scoring_list = ['neg_log_loss', 'balanced_accuracy']\n",
    "\n",
    "for ax_ind, scoring in enumerate(scoring_list):\n",
    "    LearningCurveDisplay.from_estimator(model, X_train, y_train, cv = cv,\n",
    "                                        train_sizes=np.linspace(0.05, 1, 20),\n",
    "                                        n_jobs=-1, scoring=scoring,\n",
    "                                        shuffle=True, ax=ax[ax_ind]\n",
    "                                       )\n",
    "fig.suptitle(\"Learning curves\", fontsize = 20)\n",
    "fig.tight_layout()\n",
    "\n",
    "print(\"Done\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5e69690f",
   "metadata": {},
   "source": [
    "    From this figure we can conclude that adding more training data would not help improving our model perfomance much. This is due to the fact that with increasment of training samples both lines corresponding to train and test (validation) start being very close and almost parallel to each other. We can confidently assume that in any case the mean test score will not overcome -0.355 negative log loss mark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e0f4af",
   "metadata": {},
   "source": [
    "    This approch of the lines to each other signalizes that the model is not badly overfitted, still it is hard to say whether we achieved adequate level of its' complexity (balanced accuracy score isn't really impressive). What if our model somewhat biased (underfitted)? We could try increasing perfomance by introducing polynomial features."
   ]
  },
  {
   "cell_type": "code",
   "id": "f3de8318",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "150fac9e",
   "metadata": {},
   "source": [
    "poly = PolynomialFeatures(include_bias=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "253cc394",
   "metadata": {},
   "source": [
    "cols_to_poly = ['total_nights', 'total_guests', 'room_assigned_equal_reserved',\n",
    "       'deposit_type', 'lead_time', 'previous_cancellations',\n",
    "       'previous_bookings_not_canceled', 'booking_changes', 'adr',\n",
    "       'required_car_parking_spaces', 'total_of_special_requests']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8305bfd2",
   "metadata": {},
   "source": [
    "poly_trans = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('poly', poly, cols_to_poly)\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    verbose_feature_names_out=False\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6306f3b4",
   "metadata": {},
   "source": [
    "# getting rid of spaces in new polynomial features\n",
    "def replace_gaps(df):\n",
    "    for col in df.columns:\n",
    "        df.rename({col: col.replace(' ', '*')}, axis = 1, inplace = True)\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9a7eb8d7",
   "metadata": {},
   "source": [
    "# example\n",
    "X_poly = poly.fit_transform(X_train_transformed[['lead_time', 'adr', 'total_nights']])\n",
    "\n",
    "fig, ax = plt.subplots(3,3, figsize = (16,16))\n",
    "\n",
    "for i, col in enumerate(X_poly.columns):\n",
    "    sns.histplot(data = X_poly, x = col, ax = ax[i//3, i%3], stat='density', kde=True)\n",
    "\n",
    "fig.tight_layout()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "810be7b1",
   "metadata": {},
   "source": [
    "    We will apply log transform to all features containing lead_time or adr in their names --> we need to form the list of such features."
   ]
  },
  {
   "cell_type": "code",
   "id": "5f3134d5",
   "metadata": {},
   "source": [
    "intermediate_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('one_hot_encoder', one_hot_encoder, low_cardinality_feats),\n",
    "        ('target_encoder', target_encoder, high_cardinality_feats)\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    verbose_feature_names_out=False\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5ad910db",
   "metadata": {},
   "source": [
    "intermediate_model = Pipeline(\n",
    "    steps=[\n",
    "        ('Transform_pipeline', Transform_Pipe),\n",
    "        ('poly_trans', poly_trans),\n",
    "        ('replace_gaps', FunctionTransformer(replace_gaps)),\n",
    "        ('preprocessor', intermediate_preprocessor),\n",
    "        ('debug', FunctionTransformer(debug_transformer, kw_args={'name': 'tmp_X'})),\n",
    "        ('model', LogisticRegression(max_iter=300))\n",
    "    ])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0375fff7",
   "metadata": {},
   "source": [
    "intermediate_model.fit(X_train,y_train)\n",
    "print('Fitting done')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7aa47e66",
   "metadata": {},
   "source": [
    "# features to be log-scaled\n",
    "log_list = []\n",
    "for col in ['lead_time', 'adr']:\n",
    "    l1 = [obj for obj in tmp_X.columns if col in obj]\n",
    "    log_list.extend(l1)\n",
    "log_list = list(set(log_list))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ec500292",
   "metadata": {},
   "source": [
    "preprocessor_poly = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('one_hot_encoder', one_hot_encoder, low_cardinality_feats),\n",
    "        ('target_encoder', target_encoder, high_cardinality_feats),\n",
    "        ('outliers_replace', FunctionTransformer(outlier_func), outlier_columns),\n",
    "        ('log_transform', FunctionTransformer(log_transform), log_list),\n",
    "        ('binning', binner, ['agent'])\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    verbose_feature_names_out=False\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "82b7ae35",
   "metadata": {},
   "source": [
    "not_scale_cols = ['hotel_Resort Hotel', 'stays_format_just_week', 'stays_format_just_weekend', \n",
    "                  'stays_format_undefined', 'country', 'meal_FB', 'meal_HB', 'meal_SC', 'meal_Undefined', \n",
    "                  'market_segment_Complementary', 'market_segment_Corporate', 'market_segment_Direct', \n",
    "                  'market_segment_Groups', 'market_segment_Offline TA/TO', 'market_segment_Online TA', \n",
    "                  'market_segment_Undefined', 'distribution_channel_Direct', 'distribution_channel_GDS', \n",
    "                  'distribution_channel_TA/TO', 'distribution_channel_Undefined', 'customer_type_Group', \n",
    "                  'customer_type_Transient', 'customer_type_Transient-Party']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f3a3898d",
   "metadata": {},
   "source": "cols_scale_minmax = ['total_nights', 'total_guests', 'agent', 'total_of_special_requests']",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bdd5f463",
   "metadata": {},
   "source": [
    "cols_scale_std = tmp_X.columns.difference(not_scale_cols).difference(cols_scale_minmax)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d3149060",
   "metadata": {},
   "source": [
    "scaler_poly = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('std_scaler', std_scaler, cols_scale_std),\n",
    "        ('min_max_scaler', min_max_scaler, cols_scale_minmax)\n",
    "    ],\n",
    "    remainder = 'passthrough'\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "208cbbf0",
   "metadata": {},
   "source": [
    "log_reg_poly = Pipeline(\n",
    "    steps=[\n",
    "        ('Transform_pipeline', Transform_Pipe),\n",
    "        ('poly_trans', poly_trans),\n",
    "        ('replace_gaps', FunctionTransformer(replace_gaps)),\n",
    "        ('preprocessor', preprocessor_poly),\n",
    "        ('scaler', scaler_poly),\n",
    "        ('debug', FunctionTransformer(debug_transformer, kw_args={'name': 'tmp_X'})),\n",
    "        ('model', LogisticRegression(max_iter=400))\n",
    "    ])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5af5eaec",
   "metadata": {},
   "source": [
    "log_reg_poly_nestedCV = nested_cv(log_reg_poly)\n",
    "log_reg_poly_nestedCV"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "455e4481",
   "metadata": {},
   "source": [
    "cv = StratifiedKFold()\n",
    "gscv = GridSearchCV(log_reg_poly, param_grid=param_grid, cv = cv, n_jobs=-1, scoring='neg_log_loss')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c0b29b96",
   "metadata": {},
   "source": [
    "gscv.fit(X_train, y_train)\n",
    "print(\"Done\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "28badedb",
   "metadata": {},
   "source": [
    "model = gscv.best_estimator_"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1d858eb7",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import LearningCurveDisplay"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e09f60aa",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(14,6))\n",
    "scoring_list = ['neg_log_loss', 'balanced_accuracy']\n",
    "\n",
    "for ax_ind, scoring in enumerate(scoring_list):\n",
    "    LearningCurveDisplay.from_estimator(model, X_train, y_train, cv = cv,\n",
    "                                        train_sizes=np.linspace(0.05, 1, 20),\n",
    "                                        n_jobs=-1, scoring=scoring,\n",
    "                                        shuffle=True, ax=ax[ax_ind]\n",
    "                                       )\n",
    "fig.suptitle(\"Learning curves\", fontsize=20)\n",
    "fig.tight_layout()\n",
    "\n",
    "print(\"Done\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dbde96ce",
   "metadata": {},
   "source": [
    "    From the cv_table and both learning curves figures we observe a slight improvement of our metric scores, however the number of features in dataset and fitting time have risen significantly. Thus, we will stick to the model log_reg2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62076a4",
   "metadata": {},
   "source": [
    "    Now let us explore if feature selection would be helpful for our model. We will focus on the coefficients learned by it. "
   ]
  },
  {
   "cell_type": "code",
   "id": "16d6c8e7",
   "metadata": {},
   "source": [
    "gscv = GridSearchCV(log_reg2, param_grid=param_grid, cv = cv, n_jobs=-1, scoring='neg_log_loss')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "007c5cf9",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "gscv.fit(X_train, y_train)\n",
    "print('Fitting done')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "806dd8ca",
   "metadata": {},
   "source": [
    "best_log_reg2 = gscv.best_estimator_"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b05d0af7",
   "metadata": {},
   "source": [
    "best_param = gscv.best_params_['model__C']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "40039f43",
   "metadata": {},
   "source": [
    "res_log_reg = best_log_reg2.named_steps['model']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "66afc5cd",
   "metadata": {},
   "source": [
    "coef_series = pd.Series(res_log_reg.coef_.squeeze(), index=res_log_reg.feature_names_in_)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fe364e5c",
   "metadata": {},
   "source": [
    "plt.figure(figsize = (16,12))\n",
    "coef_series.sort_values(key=abs).plot.barh()\n",
    "plt.title('Magnitude of coefficients', fontsize = 24)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "11b7ecd3",
   "metadata": {},
   "source": [
    "    From this picture we observe that coefficients of some features differ drastically from other in magnitude, i.e. some features have more impact than others. We are going to find out if we have redundant features, which we can exclude to improve the perfomance."
   ]
  },
  {
   "cell_type": "code",
   "id": "7094dc38",
   "metadata": {},
   "source": [
    "sorted_col_names = list(coef_series.sort_values(key=abs).index)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "17f8dea0",
   "metadata": {},
   "source": [
    "def drop_cols(df):\n",
    "    copy_drop_list = drop_list.copy()\n",
    "    for name in copy_drop_list:\n",
    "        if name not in df.columns:\n",
    "            copy_drop_list.remove(name)\n",
    "    return df.drop(copy_drop_list, axis=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9b0d9615",
   "metadata": {},
   "source": [
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('Transform_pipeline', Transform_Pipe),\n",
    "        ('preprocessor', preprocessor2),\n",
    "        ('scaler', scaler2),\n",
    "        ('drop_cols', FunctionTransformer(drop_cols)),\n",
    "        ('debug', FunctionTransformer(debug_transformer, kw_args={'name': 'tmp_X'})),\n",
    "        ('model', LogisticRegression(max_iter=200, C=best_param))\n",
    "    ])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "62435cdf",
   "metadata": {},
   "source": [
    "train_scores = []\n",
    "val_scores = []"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2e57b6bb",
   "metadata": {},
   "source": [
    "for i in range(1, len(sorted_col_names)):\n",
    "    drop_list = sorted_col_names[:i]\n",
    "    cv_scores = cross_validate(pipeline, X_train, y_train,\n",
    "                           scoring='balanced_accuracy', n_jobs=-1, cv=cv, return_train_score=True)\n",
    "    mean_train_score = cv_scores['train_score'].mean()\n",
    "    mean_val_score = cv_scores['test_score'].mean()\n",
    "    train_scores.append(mean_train_score)\n",
    "    val_scores.append(mean_val_score)\n",
    "    \n",
    "print(\"Done\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3acd1569",
   "metadata": {},
   "source": [
    "scores_df = pd.DataFrame({'train_scores':  train_scores, 'val_scores': val_scores},\n",
    "                          index = np.arange(1, len(train_scores)+1))\n",
    "\n",
    "scores_df.index.name = 'No_excluded'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bdd8800f",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(1,1, figsize = (8,5))\n",
    "scores_df.plot(ax = ax)\n",
    "plt.ylabel('Balanced accuracy')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c9807e6b",
   "metadata": {},
   "source": [
    "import plotly.express as px"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "16d3e7b4",
   "metadata": {},
   "source": [
    "scores_df.reset_index(inplace = True)\n",
    "px.line(data_frame=scores_df, x = 'No_excluded', y=['train_scores', 'val_scores'],\n",
    "        labels={'value': 'Balanced accuracy'})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "78c79cc4",
   "metadata": {},
   "source": [
    "    The line chart above reflects dependency between the number of excluded features and the balanced accuracy score across train and test set (here we exclude n features with the least magnitudes of assigned coefficients). Thanks to interactivity of the plot we conclude the following: despite we don't get any increasement in scores we don't get almost any loss in balanced accuracy till the number of excluded features reaches 20. Let us perform nested cross validation leaving out 19 least significant features. "
   ]
  },
  {
   "cell_type": "code",
   "id": "a1f8db9e",
   "metadata": {},
   "source": [
    "drop_list = sorted_col_names[:19]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d266c0f0",
   "metadata": {},
   "source": [
    "log_reg3 = Pipeline(\n",
    "    steps=[\n",
    "        ('Transform_pipeline', Transform_Pipe),\n",
    "        ('preprocessor', preprocessor2),\n",
    "        ('scaler', scaler2),\n",
    "        ('drop_cols', FunctionTransformer(drop_cols)),\n",
    "        ('debug', FunctionTransformer(debug_transformer, kw_args={'name': 'tmp_X'})),\n",
    "        ('model', LogisticRegression(max_iter=200))\n",
    "    ])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8a3ee465",
   "metadata": {},
   "source": [
    "log_reg3_nestedCV = nested_cv(log_reg3)\n",
    "log_reg3_nestedCV"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ee75e297",
   "metadata": {},
   "source": [
    "log_reg2_nestedCV"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8f2796a1",
   "metadata": {},
   "source": [
    "    We can notice a slight decreasement of all metric scores for log_reg3 compared to log_reg2. However we reduced the number of features by half, what improves interpretability and computational efficiency of our model. I would consider this as a positive achievement. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1277e954",
   "metadata": {},
   "source": [
    "    Finally let us perform tuning of decision threshold. The default threshold (probability estimate of 0.5) might not be optimal for maximizing a certain metric. As before, we continue to treat both classes as equally important, making attempts to increase such metrics as balanced accuracy and f1-score."
   ]
  },
  {
   "cell_type": "code",
   "id": "c8e6712d",
   "metadata": {},
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.metrics import PrecisionRecallDisplay"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6e4b5ccf",
   "metadata": {},
   "source": [
    "#### Finding out and setting the best C for log_reg3 and fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "id": "7fa5351b",
   "metadata": {},
   "source": [
    "gscv = GridSearchCV(log_reg3, param_grid=param_grid, cv = cv, n_jobs=-1, scoring='neg_log_loss')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aac0cf62",
   "metadata": {},
   "source": [
    "gscv.fit(X_train, y_train)\n",
    "print('Fitting done')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e814b91d",
   "metadata": {},
   "source": [
    "best_C = gscv.best_params_['model__C']\n",
    "\n",
    "params = {'model__C': best_C}\n",
    "\n",
    "log_reg3.set_params(**params)\n",
    "print('Parameter set')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "986cf4fc",
   "metadata": {},
   "source": [
    "log_reg3.fit(X_train, y_train)\n",
    "print('Fitting done')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d4e60535",
   "metadata": {},
   "source": [
    "#### Finding out fpr and tpr values corresponding to default threshold "
   ]
  },
  {
   "cell_type": "code",
   "id": "357a7b9c",
   "metadata": {},
   "source": [
    "y_pred_proba_orig = log_reg3.predict_proba(X_train)[:,1]\n",
    "\n",
    "fpr, tpr, roc_thresholds = roc_curve(y_train, y_pred_proba_orig)\n",
    "pr, rec, pr_rec_thresholds = precision_recall_curve(y_train, y_pred_proba_orig)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ebe51c84",
   "metadata": {},
   "source": [
    "roc_thresh_05_arg = np.abs(roc_thresholds - 0.5).argmin()\n",
    "pr_rec_thresh_05_arg = np.abs(pr_rec_thresholds - 0.5).argmin()\n",
    "\n",
    "fpr_05 = fpr[roc_thresh_05_arg]\n",
    "tpr_05 = tpr[roc_thresh_05_arg]\n",
    "\n",
    "pr_05 = pr[pr_rec_thresh_05_arg]\n",
    "rec_05 = rec[pr_rec_thresh_05_arg]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9ae19671",
   "metadata": {},
   "source": [
    "#### Applying TunedThresholdClassifierCV to find the best threshold maximizing balanced accuracy"
   ]
  },
  {
   "cell_type": "code",
   "id": "d25c34e8",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import TunedThresholdClassifierCV"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3915e2d7",
   "metadata": {},
   "source": [
    "cv = StratifiedKFold(shuffle=True, random_state=123)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "117985b8",
   "metadata": {},
   "source": [
    "tuned_model = TunedThresholdClassifierCV(log_reg3, cv=cv, n_jobs=-1,\n",
    "                                         scoring='balanced_accuracy'\n",
    "                                         #store_cv_results=True\n",
    "                                        )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c9404312",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "tuned_model.fit(X_train, y_train)\n",
    "print('Fitting done')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d22aa610",
   "metadata": {},
   "source": [
    "roc_thresh_tuned_arg = np.abs(roc_thresholds - tuned_model.best_threshold_).argmin()\n",
    "pr_rec_thresh_tuned_arg = np.abs(pr_rec_thresholds - tuned_model.best_threshold_).argmin()\n",
    "\n",
    "fpr_thresh_tuned = fpr[roc_thresh_tuned_arg]\n",
    "tpr_thresh_tuned = tpr[roc_thresh_tuned_arg]\n",
    "\n",
    "pr_thresh_tuned = pr[pr_rec_thresh_tuned_arg]\n",
    "rec_thresh_tuned = rec[pr_rec_thresh_tuned_arg]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a01b07ee",
   "metadata": {},
   "source": [
    "#### Building ROC and PrecisionRecall curves for visualization"
   ]
  },
  {
   "cell_type": "code",
   "id": "200686ce",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize = (14,8))\n",
    "\n",
    "# plotting ROC Curve\n",
    "roc = RocCurveDisplay.from_estimator(\n",
    "    log_reg3,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    name=\"log_reg3\",\n",
    "    plot_chance_level=True,\n",
    "    linewidth=2,\n",
    "    ax=ax1,\n",
    ")\n",
    "\n",
    "roc.ax_.plot(\n",
    "    fpr_05, tpr_05, \n",
    "    marker='o', \n",
    "    c = 'orange', \n",
    "    markersize=10,\n",
    "    label='Threshold 0.5'\n",
    ")\n",
    "\n",
    "roc.ax_.plot(\n",
    "    fpr_thresh_tuned, tpr_thresh_tuned, \n",
    "    marker='X', \n",
    "    c = 'black', \n",
    "    markersize=10,\n",
    "    label= f\"Threshold{tuned_model.best_threshold_: .3f}\"\n",
    ")\n",
    "\n",
    "roc.ax_.set_xlabel('False Positive Rate')\n",
    "roc.ax_.set_ylabel('True Positive Rate')\n",
    "roc.ax_.set_title('ROC curve')\n",
    "roc.ax_.legend()\n",
    "\n",
    "# plotting PrecisonRecall Curve\n",
    "pr_rec = PrecisionRecallDisplay.from_estimator(\n",
    "    log_reg3,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    name = \"log_reg3\",\n",
    "    plot_chance_level=True,\n",
    "    linewidth=2,\n",
    "    ax=ax2\n",
    ")\n",
    "\n",
    "pr_rec.ax_.plot(\n",
    "    rec_05, pr_05, \n",
    "    marker='o', \n",
    "    c = 'orange', \n",
    "    markersize=10,\n",
    "    label='Threshold 0.5'\n",
    ")\n",
    "\n",
    "pr_rec.ax_.plot(\n",
    "    rec_thresh_tuned, pr_thresh_tuned, \n",
    "    marker='X', \n",
    "    c = 'black', \n",
    "    markersize=10,\n",
    "    label= f\"Threshold{tuned_model.best_threshold_: .3f}\"\n",
    ")\n",
    "\n",
    "pr_rec.ax_.set_xlabel('Recall')\n",
    "pr_rec.ax_.set_ylabel('Precision')\n",
    "pr_rec.ax_.set_title('PresisionRecall Curve')\n",
    "pr_rec.ax_.legend()\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6de9ea71",
   "metadata": {},
   "source": [
    "#### Comparing perfomances of original and tuned models"
   ]
  },
  {
   "cell_type": "code",
   "id": "279bad70",
   "metadata": {},
   "source": [
    "y_pred_orig = log_reg3.predict(X_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8255f21a",
   "metadata": {},
   "source": [
    "b_acc_score_orig = balanced_accuracy_score(y_test, y_pred_orig)\n",
    "print(f\"Balanced accuracy score with 0.5 (default) threshold: {b_acc_score_orig: .4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "af6efb12",
   "metadata": {},
   "source": [
    "y_pred_tuned = tuned_model.predict(X_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "112ca4cf",
   "metadata": {},
   "source": [
    "b_acc_score_tuned = balanced_accuracy_score(y_test, y_pred_tuned)\n",
    "print(f\"Balanced accuracy score with {tuned_model.best_threshold_:.3f} (tuned) threshold: {b_acc_score_tuned: .4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "20ff5694",
   "metadata": {},
   "source": [
    "    In general balanced accuracy is the mean of TPR (TruePositiveRate) and TNR (TrueNegativeRate). In turn the latter equals to 1 - FPR, where FPR is FalsePositiveRate. With that said, it totally makes sense that the best balanced accuracy score is achieved when we establish the best trade-off between TPR and FPR. On the ROC curve this condition is reflected by the point, which is the closest to the upper-left corner of the plot. Thus, it is natural that TunedThresholdClassifierCV found new threshold which corresponds to such trade-off (depicted by black cross on the ROC curve display). Regarding the PrecisionRecall curve, location of marks also makes sense. The lower the precision, the lower is TNR, but we also obtained higher recall (TPR), which rose more significantly (visually from the plot). In other words, precision-recall trade-off was found."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd83077",
   "metadata": {},
   "source": [
    "    In order to get robust estimate of tuned model's performance we again resort to NestedCV."
   ]
  },
  {
   "cell_type": "code",
   "id": "2ce1da5e",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c0ab2268",
   "metadata": {},
   "source": [
    "inner_cv = StratifiedKFold(n_splits=3)\n",
    "outer_cv = RepeatedStratifiedKFold(n_splits=4, n_repeats=20, random_state=123)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0dfd551c",
   "metadata": {},
   "source": [
    "#### Maximizing balanced accuracy"
   ]
  },
  {
   "cell_type": "code",
   "id": "2b14254e",
   "metadata": {},
   "source": [
    "tuned_log_b_acc = TunedThresholdClassifierCV(log_reg3, cv=inner_cv, n_jobs=-1,\n",
    "                                             scoring='balanced_accuracy'\n",
    "                                            )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d2b2bfc9",
   "metadata": {},
   "source": [
    "cv_scores_b_acc = cross_validate(tuned_log_b_acc, X, y, cv = outer_cv, n_jobs=-1,\n",
    "                                 scoring = ['balanced_accuracy', 'precision', 'recall', 'f1'],\n",
    "                                 return_estimator=True\n",
    "                                )\n",
    "\n",
    "print(\"Done\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a2cbcc16",
   "metadata": {},
   "source": [
    "test_scores = ['test_balanced_accuracy', 'test_precision', 'test_recall', 'test_f1']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3698b630",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "tuned_b_acc_scores = pd.DataFrame(cv_scores_b_acc)[test_scores].agg(['mean', 'std']).T\n",
    "tuned_b_acc_scores"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ca41c184",
   "metadata": {},
   "source": [
    "#### Maximizing f1-score"
   ]
  },
  {
   "cell_type": "code",
   "id": "7c4a0957",
   "metadata": {},
   "source": [
    "tuned_log_f1 = TunedThresholdClassifierCV(log_reg3, cv=inner_cv, n_jobs=-1,\n",
    "                                          scoring='f1'\n",
    "                                         )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "350a08ce",
   "metadata": {},
   "source": [
    "cv_scores_f1 = cross_validate(tuned_log_f1, X, y, cv = outer_cv, n_jobs=-1,\n",
    "                              scoring = ['balanced_accuracy', 'precision', 'recall', 'f1'],\n",
    "                              return_estimator=True\n",
    "                             )\n",
    "\n",
    "print(\"Done\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a4b2accd",
   "metadata": {},
   "source": [
    "tuned_f1_scores = pd.DataFrame(cv_scores_f1)[test_scores].agg(['mean', 'std']).T\n",
    "tuned_f1_scores"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ae261fbd",
   "metadata": {},
   "source": [
    "    We considered two scenarios: maximizing balanced accuracy and then same for f1-score. As a result, we obtained relatively similar results. However, we still opt for balanced accuracy as scoring metric for TunedThresholdClassifierCV as it showed slightly better performance."
   ]
  },
  {
   "cell_type": "code",
   "id": "1a1634ed",
   "metadata": {},
   "source": [
    "decision_thresh = pd.Series(\n",
    "    [est.best_threshold_ for est in cv_scores_b_acc['estimator']]\n",
    "                           )\n",
    "mean_thresh = decision_thresh.mean()\n",
    "ax = decision_thresh.plot.kde()\n",
    "\n",
    "ax.axvline(\n",
    "    mean_thresh,\n",
    "    color=\"k\",\n",
    "    linestyle=\"--\",\n",
    "    label=f\"Mean decision threshold: {mean_thresh:.3f}\",\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Decision threshold\")\n",
    "ax.legend(loc=\"upper right\", fontsize=9)\n",
    "ax.set_title(\"Distribution of the decision threshold across different cross-validation folds\", fontsize=12)\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a7c679ab",
   "metadata": {},
   "source": "    In average, a decision threshold around 0.345 maximizes the balanced accuracy. Let us make our final model: we just fix a new cut-off point for the decision function of log_reg3."
  },
  {
   "cell_type": "code",
   "id": "a7fe0f9a",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import FixedThresholdClassifier"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "269dd150",
   "metadata": {},
   "source": [
    "final_log_reg = FixedThresholdClassifier(log_reg3, threshold=mean_thresh)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "556b1f7b",
   "metadata": {},
   "source": [
    "final_log_reg.fit(X_train, y_train)\n",
    "y_pred_final = final_log_reg.predict(X_test)\n",
    "b_acc_final = balanced_accuracy_score(y_test, y_pred_final)\n",
    "print(f\"Applying final model we obtain balanced accuracy of {b_acc_final:.4f} on the test set\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f3cd7f4e",
   "metadata": {},
   "source": [
    "    It only remains to fit the final model on the whole data."
   ]
  },
  {
   "cell_type": "code",
   "id": "fb77c702",
   "metadata": {},
   "source": [
    "final_log_reg.fit(X, y)\n",
    "print(\"Final fitting done!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dc3ed5a2",
   "metadata": {},
   "source": "|#### Another personal remark concearning the significance of precision and recall for this particular classification problem:"
  },
  {
   "cell_type": "markdown",
   "id": "57479893",
   "metadata": {},
   "source": [
    "If we knew exactly (or at least approximately) how much we gain (lose) making TP, FP, FN, TN predictions, respectively, we could create a metric of our own and maximize it by finding the most suitable threshold. \n",
    "\n",
    "Still, from my personal perspective, it seems that false positives (predicting that there will be a cancellation when in reality not) are more dangerous than false negatives: empty room and related losses are not as important as reputation of a hotel (imagine if guest still comes and there is no room left for him as certain measures were taken basing on false positive prediction). \n",
    "\n",
    "As alternative to cost-sensitive learning, we could determine the maximum FalsePositiveRate, which we consider justified, say 5%:"
   ]
  },
  {
   "cell_type": "code",
   "id": "39d06de7",
   "metadata": {},
   "source": [
    "from sklearn.metrics import make_scorer"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f2651354",
   "metadata": {},
   "source": [
    "def max_tpr_at_fpr_constraint(y_true, y_pred, max_fpr=0.5):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    tpr_at_fpr_constraint = tpr[fpr <= max_fpr].max()\n",
    "    return tpr_at_fpr_constraint"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f821f941",
   "metadata": {},
   "source": [
    "max_fpr = 0.05"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d9e11610",
   "metadata": {},
   "source": [
    "max_tpr_at_fpr_005 = make_scorer(max_tpr_at_fpr_constraint, max_fpr=max_fpr)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d57b885c",
   "metadata": {},
   "source": [
    "cv = StratifiedKFold(shuffle=True, random_state=123)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9e3e6d9e",
   "metadata": {},
   "source": [
    "tuned_model = TunedThresholdClassifierCV(log_reg3, cv=cv, n_jobs=-1,\n",
    "                                         scoring = max_tpr_at_fpr_005\n",
    "                                         #store_cv_results=True\n",
    "                                        )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "78f7a02d",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "tuned_model.fit(X_train, y_train)\n",
    "print('Fitting done')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1a33072c",
   "metadata": {},
   "source": [
    "roc_thresh_tuned_arg = np.abs(roc_thresholds - tuned_model.best_threshold_).argmin()\n",
    "pr_rec_thresh_tuned_arg = np.abs(pr_rec_thresholds - tuned_model.best_threshold_).argmin()\n",
    "\n",
    "fpr_thresh_tuned = fpr[roc_thresh_tuned_arg]\n",
    "tpr_thresh_tuned = tpr[roc_thresh_tuned_arg]\n",
    "\n",
    "pr_thresh_tuned = pr[pr_rec_thresh_tuned_arg]\n",
    "rec_thresh_tuned = rec[pr_rec_thresh_tuned_arg]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "073f665d",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize = (14,8))\n",
    "\n",
    "# plotting ROC Curve\n",
    "roc = RocCurveDisplay.from_estimator(\n",
    "    log_reg3,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    name=\"log_reg3\",\n",
    "    plot_chance_level=True,\n",
    "    linewidth=2,\n",
    "    ax=ax1,\n",
    ")\n",
    "\n",
    "roc.ax_.plot(\n",
    "    fpr_05, tpr_05, \n",
    "    marker='o', \n",
    "    c = 'orange', \n",
    "    markersize=10,\n",
    "    label='Threshold 0.5'\n",
    ")\n",
    "\n",
    "roc.ax_.plot(\n",
    "    fpr_thresh_tuned, tpr_thresh_tuned, \n",
    "    marker='X', \n",
    "    c = 'black', \n",
    "    markersize=10,\n",
    "    label= f\"Threshold{tuned_model.best_threshold_: .3f}\"\n",
    ")\n",
    "\n",
    "roc.ax_.set_xlabel('False Positive Rate')\n",
    "roc.ax_.set_ylabel('True Positive Rate')\n",
    "roc.ax_.set_title('ROC curve')\n",
    "roc.ax_.legend()\n",
    "\n",
    "# plotting PrecisonRecall Curve\n",
    "pr_rec = PrecisionRecallDisplay.from_estimator(\n",
    "    log_reg3,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    name = \"log_reg3\",\n",
    "    plot_chance_level=True,\n",
    "    linewidth=2,\n",
    "    ax=ax2\n",
    ")\n",
    "\n",
    "pr_rec.ax_.plot(\n",
    "    rec_05, pr_05, \n",
    "    marker='o', \n",
    "    c = 'orange', \n",
    "    markersize=10,\n",
    "    label='Threshold 0.5'\n",
    ")\n",
    "\n",
    "pr_rec.ax_.plot(\n",
    "    rec_thresh_tuned, pr_thresh_tuned, \n",
    "    marker='X', \n",
    "    c = 'black', \n",
    "    markersize=10,\n",
    "    label= f\"Threshold{tuned_model.best_threshold_: .3f}\"\n",
    ")\n",
    "\n",
    "pr_rec.ax_.set_xlabel('Recall')\n",
    "pr_rec.ax_.set_ylabel('Precision')\n",
    "pr_rec.ax_.set_title('PresisionRecall Curve')\n",
    "pr_rec.ax_.legend()\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b3fedf54",
   "metadata": {},
   "source": [
    "inner_cv = StratifiedKFold(n_splits=3)\n",
    "outer_cv = RepeatedStratifiedKFold(n_splits=4, n_repeats=20, random_state=123)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1cd4d9de",
   "metadata": {},
   "source": [
    "tuned_fpr_constraint = TunedThresholdClassifierCV(log_reg3, cv=inner_cv, n_jobs=-1,\n",
    "                                                  scoring=max_tpr_at_fpr_005\n",
    "                                                 )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0cbe31b0",
   "metadata": {},
   "source": [
    "cv_scores = cross_validate(tuned_fpr_constraint, X, y, cv = outer_cv, n_jobs=-1,\n",
    "                           scoring = ['balanced_accuracy', 'precision', 'recall', 'f1'],\n",
    "                           return_estimator=True\n",
    "                          )\n",
    "\n",
    "print(\"Done\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2de42833",
   "metadata": {},
   "source": [
    "pd.DataFrame(cv_scores)[test_scores].agg(['mean', 'std']).T"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fa79d77c",
   "metadata": {},
   "source": [
    "The results are natural: setting FalsePositiveRate low, we obtain higher precision score. Although, it comes at a cost of lower recall score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355babd7",
   "metadata": {},
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "id": "3dec43c9",
   "metadata": {},
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "from sklearn.metrics import make_scorer"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ab242b7a",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import FixedThresholdClassifier\n",
    "from sklearn.model_selection import LearningCurveDisplay\n",
    "from sklearn.model_selection import TunedThresholdClassifierCV"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5fa0e8ff",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "90fda466",
   "metadata": {},
   "source": [
    "from time import time\n",
    "import plotly.express as px"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3644f1e4",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore:DataFrameGroupBy.apply operated on the grouping columns:DeprecationWarning,\\\n",
    "                                ignore:Bins whose width are too small:UserWarning,\\\n",
    "                                ignore:invalid value encountered in log1p:RuntimeWarning,\\\n",
    "                                ignore:Found unknown categories in columns:UserWarning'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9b389c68",
   "metadata": {},
   "source": [
    "    Tree-based models are known for their ability to capture non-linear relationships in dataset. A big advantage ot these algorithms is that they do not require much data preprocessing like feature engineering, scaling or normalization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d476d724",
   "metadata": {},
   "source": [
    "    In the following section application of RandomForestClassifier is carried out. This is a bagging ensemble method with DecisionTreeClassifier as a base model. A single decision tree is prone to overfitting. Random forest, in turn, involves training multiple decision trees on different subsets of the training data and then averaging their predictions. That makes it robust to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "id": "ea9dd60a",
   "metadata": {},
   "source": [
    "high_cardinality_feats = ['country']\n",
    "low_cardinality_feats = ['customer_type', 'distribution_channel', 'hotel', \n",
    "                         'market_segment', 'meal', 'stays_format']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "75dd0c59",
   "metadata": {},
   "source": [
    "label_encoder = LabelEncoder()\n",
    "target_encoder = TargetEncoder(target_type='binary', random_state=142)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "466ca941",
   "metadata": {},
   "source": [
    "def Le_Func(df):\n",
    "    for col in df.columns:\n",
    "        df[col] = label_encoder.fit_transform(df[col])\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "64471ed0",
   "metadata": {},
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('label_encoder', FunctionTransformer(Le_Func), low_cardinality_feats),\n",
    "        ('target_encoder', target_encoder, high_cardinality_feats)\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    verbose_feature_names_out=False\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "926f785a",
   "metadata": {},
   "source": [
    "rf_pipe = Pipeline(\n",
    "    steps=[\n",
    "        ('Transform_pipeline', Transform_Pipe),\n",
    "        ('preprocessor', preprocessor),\n",
    "        # in this step we can obtain the result of transform by ETL_pipeline and preprocessor \n",
    "        ('debug', FunctionTransformer(debug_transformer, kw_args={'name': 'tmp_X'})),\n",
    "        ('model', RandomForestClassifier(n_jobs=-1))\n",
    "    ])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4d94e472",
   "metadata": {},
   "source": [
    "rf_pipe.fit(X_train, y_train)\n",
    "print('Fitting done')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6774b5f7",
   "metadata": {},
   "source": [
    "init_train_score = balanced_accuracy_score(rf_pipe.predict(X_train), y_train)\n",
    "print(f\"Balanced accuracy on the training set: {init_train_score:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c9c40a8a",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "init_test_score = balanced_accuracy_score(rf_pipe.predict(X_test), y_test)\n",
    "print(f\"Balanced accuracy on the test set: {init_test_score:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4c13c2b5",
   "metadata": {},
   "source": [
    "    Very high train score and relatively low test score - sign of overfitting. We fix random_state to ensure consistency of results."
   ]
  },
  {
   "cell_type": "code",
   "id": "ce88aea8",
   "metadata": {},
   "source": [
    "rf_pipe.set_params(**{'model__random_state': 123})\n",
    "print('Setting done')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a315ae26",
   "metadata": {},
   "source": [
    "    Right now each decision tree uses the whole training dataset to learn on. Moreover, the trees are not limited in growth. These aspects lead to overfitting of each single tree and, consequently, our ensemble. Supposably, if we weaken our learners, we will decrease variance of the model, in other words the model will generalize better. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93028cbc",
   "metadata": {},
   "source": [
    "    The influence of a single hyperparameter on the training score and validation score can be demonstrated by validation curves. What if we decrease a size of random training sample for each tree?"
   ]
  },
  {
   "cell_type": "code",
   "id": "38b83d6e",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import ValidationCurveDisplay"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c3e8c97c",
   "metadata": {},
   "source": [
    "samp_sizes = np.linspace(0.05, 1, 20)\n",
    "cv = StratifiedKFold()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6689545d",
   "metadata": {},
   "source": [
    "t0 = time()\n",
    "samp_sizes_disp = ValidationCurveDisplay.from_estimator(\n",
    "    rf_pipe,\n",
    "    X_train, \n",
    "    y_train,\n",
    "    param_name='model__max_samples',\n",
    "    param_range = samp_sizes,\n",
    "    cv=cv,\n",
    "    scoring='balanced_accuracy',\n",
    "    n_jobs=-1,\n",
    "    std_display_style=None\n",
    ")\n",
    "plt.title('Validation curves')\n",
    "print(f\"Done in {time() - t0:.1f}s\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ee148f81",
   "metadata": {},
   "source": [
    "    If we set max_samples to 0.6 we won't change the validation score much, however the training score will decrease."
   ]
  },
  {
   "cell_type": "code",
   "id": "eb8c39c5",
   "metadata": {},
   "source": [
    "rf_pipe.set_params(**{'model__max_samples': 0.6})\n",
    "print(\"Setting done!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e1b090cb",
   "metadata": {},
   "source": [
    "    As for parameters responsible for the size of the trees, we will consider max_depth and ccp_alpha."
   ]
  },
  {
   "cell_type": "code",
   "id": "0f1c5033",
   "metadata": {},
   "source": [
    "depths = np.arange(1, 31, 1)\n",
    "ccp_alphas = np.linspace(0, 0.1, 30)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6956b6ea",
   "metadata": {},
   "source": [
    "t0 = time()\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize = (14,6), sharey=True)\n",
    "depth_disp = ValidationCurveDisplay.from_estimator(\n",
    "    rf_pipe,\n",
    "    X_train, \n",
    "    y_train,\n",
    "    param_name='model__max_depth',\n",
    "    param_range = depths,\n",
    "    cv=cv,\n",
    "    scoring='balanced_accuracy',\n",
    "    n_jobs=-1,\n",
    "    ax=ax1\n",
    ")\n",
    "\n",
    "ccp_disp = ValidationCurveDisplay.from_estimator(\n",
    "    rf_pipe,\n",
    "    X_train, \n",
    "    y_train,\n",
    "    param_name='model__ccp_alpha',\n",
    "    param_range = ccp_alphas,\n",
    "    cv=cv,\n",
    "    scoring='balanced_accuracy',\n",
    "    n_jobs=-1,\n",
    "    ax=ax2\n",
    ")\n",
    "fig.suptitle(\"Validation curves\", fontsize = 20)\n",
    "fig.tight_layout()\n",
    "\n",
    "print(f\"Done in {time() - t0:.1f}s\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5c8fa416",
   "metadata": {},
   "source": [
    "    We get drastic decline of both validation and train score when we start pruning. On the other hand, restricting depth of the trees to 20 does not hurt much towards validation score, while it makes train score decrease significantly."
   ]
  },
  {
   "cell_type": "code",
   "id": "505ebe34",
   "metadata": {},
   "source": [
    "rf_pipe.set_params(**{'model__max_depth': 20})\n",
    "print('Setting done!')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c17b9885",
   "metadata": {},
   "source": [
    "#### Plotting learning curves"
   ]
  },
  {
   "cell_type": "code",
   "id": "b7ada5a2",
   "metadata": {},
   "source": [
    "t0 = time()\n",
    "LearningCurveDisplay.from_estimator(rf_pipe, X_train, y_train, cv = cv,\n",
    "                                    train_sizes=np.linspace(0.05, 1, 20),\n",
    "                                    n_jobs=-1, scoring='balanced_accuracy',\n",
    "                                    shuffle=True, \n",
    "                                    score_name = 'balanced_accuracy'\n",
    "                                   )\n",
    "plt.title(\"Learning curves\")\n",
    "print(f\"Done in {time() - t0:.1f}s\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cd0f07bd",
   "metadata": {},
   "source": [
    "    After attempting to influence our model by max_samples and max_depth we still have much higher training score compared to validation score. To resolve this problem collecting more training data could be helpful, as we observe that the lines are slowly converging to each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8f7b15",
   "metadata": {},
   "source": [
    "    Another possible solution is reducing the set of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1c23c2",
   "metadata": {},
   "source": [
    "#### Feature importances (Mean Decrease in Impurity)"
   ]
  },
  {
   "cell_type": "code",
   "id": "e95f817d",
   "metadata": {},
   "source": [
    "rf_pipe.fit(X_train, y_train)\n",
    "print('Fitting done')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7bbbcc88",
   "metadata": {},
   "source": [
    "importances = rf_pipe.named_steps['model'].feature_importances_\n",
    "feature_names = rf_pipe.named_steps['model'].feature_names_in_"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b708d5a0",
   "metadata": {},
   "source": [
    "mdi_series = pd.Series(importances, index = feature_names).sort_values()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "91d14856",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "mdi_series.plot.barh()\n",
    "plt.title(\"Feature importances (MDI)\")\n",
    "plt.xlabel(\"Mean decrease in impurity\")\n",
    "plt.xticks(rotation = 20)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cfcac6e6",
   "metadata": {},
   "source": [
    "    MDI statistic has a certain drawback - it is derived from the train set only. Therefore, there is no way to ensure that particular feature would be usefull to make predictions that generalize to the unseen data. On the contrary, feature importances based on feature permutation (permutation importances) can be computed on the left-out test set and thus give us better understanding of how impactful the feature is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66100f40",
   "metadata": {},
   "source": [
    "#### Permutation importances"
   ]
  },
  {
   "cell_type": "code",
   "id": "b66e57bc",
   "metadata": {},
   "source": [
    "from sklearn.inspection import permutation_importance"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c2ce94d0",
   "metadata": {},
   "source": [
    "X_transformed = Transform_Pipe.fit_transform(X, y)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "337f6e0c",
   "metadata": {},
   "source": [
    "X_tr_train, X_tr_test, y_tr_train, y_tr_test = train_test_split(\n",
    "    X_transformed, y, test_size=0.25, random_state=123, stratify=y\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5a274de7",
   "metadata": {},
   "source": [
    "params = {'model__random_state': 123,\n",
    "          'model__n_jobs': -1,\n",
    "          'model__max_depth': 20,\n",
    "          'model__max_samples': 0.6\n",
    "         }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3b6f22cd",
   "metadata": {},
   "source": [
    "temp_pipe = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        # in this step we can obtain the result of transform by ETL_pipeline and preprocessor \n",
    "        ('debug', FunctionTransformer(debug_transformer, kw_args={'name': 'tmp_X'})),\n",
    "        ('model', RandomForestClassifier())\n",
    "    ])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "66c902f2",
   "metadata": {},
   "source": [
    "temp_pipe.set_params(**params)\n",
    "print(\"Setting done!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bde6e83b",
   "metadata": {},
   "source": [
    "temp_pipe.fit(X_tr_train, y_tr_train)\n",
    "print(\"Fitting done!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "020e2e11",
   "metadata": {},
   "source": [
    "t0 = time()\n",
    "permut = permutation_importance(\n",
    "    temp_pipe, X_tr_test, y_tr_test, n_repeats=10, \n",
    "    n_jobs=-1, random_state=123,\n",
    "    scoring = 'balanced_accuracy'\n",
    ")\n",
    "print(f\"Done in {time() - t0:.1f}s\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0c77b49d",
   "metadata": {},
   "source": [
    "perm_importances = permut.importances_mean\n",
    "\n",
    "permute_series = pd.Series(perm_importances, index = feature_names).sort_values()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "74347c4d",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "permute_series.plot.barh()\n",
    "plt.title(\"Permutation importances\")\n",
    "plt.xlabel(\"Mean accuracy decrease\")\n",
    "plt.xticks(rotation = 20)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6c316d47",
   "metadata": {},
   "source": [
    "    According to permutation importances market_segment is not rated well, whereas it got high feature importance on the train set. On the contrary, permutation importance of stays_format showed itself radically better than feature importance. However, it is worth observing that such features as deposit_type, country, total_of_special_requests find themselves in the top on both charts. They may be deemed truly significant."
   ]
  },
  {
   "cell_type": "code",
   "id": "3aa4d002",
   "metadata": {},
   "source": [
    "best_permute_feats = list(permute_series[-13:].index)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "14238afd",
   "metadata": {},
   "source": [
    "#### RFE"
   ]
  },
  {
   "cell_type": "code",
   "id": "9ec0f72a",
   "metadata": {},
   "source": [
    "from sklearn.feature_selection import RFE"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4088f184",
   "metadata": {},
   "source": [
    "    Recursive Feature Elimination is the method which aims to recursively eliminate the least contributing features until a smaller subset of features is reached."
   ]
  },
  {
   "cell_type": "code",
   "id": "7e3457a4",
   "metadata": {},
   "source": [
    "X_preprocessed = preprocessor.fit_transform(X_transformed, y)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "41e91cb2",
   "metadata": {},
   "source": [
    "X_prep_train, X_prep_test, y_prep_train, y_prep_test = train_test_split(\n",
    "    X_preprocessed, y, test_size=0.25, random_state=123, stratify=y\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1a323216",
   "metadata": {},
   "source": [
    "rf = RandomForestClassifier(random_state=123, max_depth=20, max_samples=0.6, n_jobs=-1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bc47cb7c",
   "metadata": {},
   "source": [
    "rfe = RFE(rf, n_features_to_select=13)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aecbe73c",
   "metadata": {},
   "source": [
    "t0 = time()\n",
    "rfe.fit(X_prep_train, y_prep_train)\n",
    "\n",
    "print(f\"Done in {time() - t0:.1f}s\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c22baeca",
   "metadata": {},
   "source": [
    "rfe_selected_features = list(rfe.feature_names_in_[rfe.support_])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e47cd977",
   "metadata": {},
   "source": [
    "#### Quick comparison between features with highest permutation importances and features selected by RFE"
   ]
  },
  {
   "cell_type": "code",
   "id": "39ca8f5d",
   "metadata": {},
   "source": [
    "rf.fit(X_prep_train[best_permute_feats], y_prep_train)\n",
    "print(\"Fitting done!\")\n",
    "b_acc = balanced_accuracy_score(rf.predict(X_prep_test[best_permute_feats]), y_prep_test)\n",
    "print(f\"Chosen based on permutation importances: {b_acc:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cb54de1c",
   "metadata": {},
   "source": [
    "rf.fit(X_prep_train[rfe_selected_features], y_prep_train)\n",
    "print(\"Fitting done!\")\n",
    "b_acc = balanced_accuracy_score(rf.predict(X_prep_test[rfe_selected_features]), y_prep_test)\n",
    "print(f\"Chosen based on RFE procedure: {b_acc:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9f4c4900",
   "metadata": {},
   "source": [
    "#### Modifying pipeline"
   ]
  },
  {
   "cell_type": "code",
   "id": "5dc6c561",
   "metadata": {},
   "source": [
    "def select_feats(df):\n",
    "    return df[rfe_selected_features]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5ea22af3",
   "metadata": {},
   "source": [
    "params = {'model__random_state': 123,\n",
    "          'model__n_jobs': -1,\n",
    "          'model__max_depth': 20,\n",
    "          'model__max_samples': 0.6\n",
    "         }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c913ec30",
   "metadata": {},
   "source": [
    "rf_pipe = Pipeline(\n",
    "    steps=[\n",
    "        ('Transform_pipeline', Transform_Pipe),\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('selector', FunctionTransformer(select_feats)),\n",
    "        ('debug', FunctionTransformer(debug_transformer, kw_args={'name': 'tmp_X'})),\n",
    "        ('model', RandomForestClassifier())\n",
    "    ])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "26bceb3f",
   "metadata": {},
   "source": [
    "rf_pipe.set_params(**params)\n",
    "print(\"Setting done!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "595a3f2b",
   "metadata": {},
   "source": [
    "t0 = time()\n",
    "LearningCurveDisplay.from_estimator(rf_pipe, X_train, y_train, cv = cv,\n",
    "                                    train_sizes=np.linspace(0.05, 1, 20),\n",
    "                                    n_jobs=-1, scoring='balanced_accuracy',\n",
    "                                    shuffle=True, \n",
    "                                    score_name = 'balanced_accuracy'\n",
    "                                   )\n",
    "plt.title('Learning curves')\n",
    "print(f\"Done in {time() - t0:.1f}s\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0ed7d57b",
   "metadata": {},
   "source": [
    "    As a result we made the gap between train and validation score smaller, i.e. reduced variance of the model. Although, this came at a cost - the validation score decreased a little. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69c1b1b",
   "metadata": {},
   "source": [
    "#### NestedCV"
   ]
  },
  {
   "cell_type": "code",
   "id": "3229acd9",
   "metadata": {},
   "source": [
    "outer_nest_cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=123)\n",
    "inner_nest_cv = StratifiedKFold(n_splits=3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b9aa3ab0",
   "metadata": {},
   "source": [
    "min_samp_split = [2, 3, 5, 8, 10]\n",
    "max_features = ['sqrt', 'log2', None]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "50a4bec8",
   "metadata": {},
   "source": [
    "param_grid = {'model__min_samples_split': min_samp_split,\n",
    "              'model__max_features': max_features\n",
    "             }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4c78b142",
   "metadata": {},
   "source": [
    "t0 = time()\n",
    "\n",
    "gscv = GridSearchCV(rf_pipe, param_grid=param_grid, cv = inner_nest_cv, n_jobs=-1, scoring='balanced_accuracy')\n",
    "\n",
    "cv_scores = cross_validate(gscv, X, y, cv = outer_nest_cv, n_jobs=-1, \n",
    "                           scoring=['balanced_accuracy', 'precision', 'recall', 'f1'],\n",
    "                           return_estimator=True\n",
    "                          )\n",
    "\n",
    "print(f'Fitting done in {time() - t0:.2f}s')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ab4b0d5c",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "pd.DataFrame(cv_scores).drop(['fit_time', 'score_time', 'estimator'], axis=1).agg(['mean', 'std']).T"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9a47e934",
   "metadata": {},
   "source": [
    "    We observe the test_balanced_accuracy score to be around 0.856, which is the value that we could expect on the unseen data regardless of the model generating method."
   ]
  },
  {
   "cell_type": "code",
   "id": "a2f237a3",
   "metadata": {},
   "source": [
    "gscv = GridSearchCV(rf_pipe, param_grid=param_grid, cv = inner_nest_cv, n_jobs=-1, scoring='balanced_accuracy')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1e9be390",
   "metadata": {},
   "source": [
    "gscv.fit(X_train, y_train)\n",
    "print(\"Fitting done!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "25327518",
   "metadata": {},
   "source": [
    "rf_pipe.set_params(**gscv.best_params_)\n",
    "print(\"Setting done!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2f1caa82",
   "metadata": {},
   "source": [
    "rf_pipe.fit(X_train, y_train)\n",
    "print(\"Fitting done!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7cbf7b1b",
   "metadata": {},
   "source": [
    "#### Final result"
   ]
  },
  {
   "cell_type": "code",
   "id": "b1bd948a",
   "metadata": {},
   "source": [
    "b_acc = balanced_accuracy_score(rf_pipe.predict(X_test), y_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "644f18d7",
   "metadata": {},
   "source": [
    "print(f\"We achieved balanced accuracy of {b_acc:.4f} on the test set!\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
